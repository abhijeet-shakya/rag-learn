{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e7dd8d",
   "metadata": {},
   "source": [
    "### Maximal Marginal Relevance \n",
    "MMR(Maximal Marginal Relevance) is a powerful diversity-aware retrieval technique used in information retrieval and RAG pipelines to balance relevance and novelty when selecting documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b030db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_classic.document_loaders import TextLoader\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af48c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee43cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Document Load\n",
    "loader= TextLoader(\"mmr_rag_practice_document.txt\")\n",
    "raw_docs = loader.load()\n",
    "\n",
    "# Step 2: Split the document\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ab4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Embedding Model and FAISS vector store\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "vector_store = FAISS.from_documents(chunks, embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc3a8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create MMR Retriever\n",
    "retriever=vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a4b4c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Promt and LLM\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context provided.\n",
    "\n",
    "Context:{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm = init_chat_model(\"groq:groq/compound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fbbe9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: RAG Pipeline\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "rag_chain= create_retrieval_chain(retriever=retriever, combine_docs_chain=document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e227c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Query\n",
    "query = {\"input\": \"How does Langchain support agents and memeory?\"}\n",
    "response = rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5186f567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Answer:\n",
      " **How LangChain supports agents and memory**\n",
      "\n",
      "LangChain (and its companion project LangGraph) provides a full‑stack set of primitives that let developers build **LLM‑powered agents** that can **remember** what has happened before, update that knowledge, and reuse it later. The main pieces are:\n",
      "\n",
      "| Area | What LangChain offers | How it is used |\n",
      "|------|----------------------|----------------|\n",
      "| **Agent framework** | • `AgentExecutor`, `create_agent`, `ZeroShotAgent`, tool‑calling agents, and the newer **Agentic Graph** API (via LangGraph). <br>• Built‑in support for tool integration, planning, and multi‑step reasoning. | You define a set of tools (search, DB lookup, code execution, etc.) and a prompt template; the executor orchestrates calls to the LLM, decides which tool to invoke, and stitches the results together. |\n",
      "| **Short‑term (thread‑level) memory** | • `ConversationBufferMemory`, `ConversationBufferWindowMemory`, `ConversationSummaryMemory`, etc. <br>• Memory is attached to an agent’s `state` and persisted with a **checkpointer** (e.g., `InMemorySaver`, Redis, SQLite). | The memory stores the most recent turns of a conversation so the LLM can see context without re‑sending the whole history. |\n",
      "| **Long‑term / persistent memory** | • **LangMem SDK** – a library that extracts salient facts from interactions, writes them to any backing store, and can later retrieve them as context. <br>• **Memory Store** (part of LangGraph) – a dedicated service for episodic, semantic, and procedural memories that runs in the background. | Use `LangMem` to create “facts” (e.g., user preferences, task outcomes) that survive across sessions. The Memory Store can be queried by the agent to recall past episodes or to update procedural knowledge (e.g., new rules). |\n",
      "| **Memory types** | • **Episodic memory** – stores specific interactions (who said what, when). <br>• **Semantic memory** – stores extracted facts or embeddings for fast similarity search. <br>• **Procedural memory** – stores updated prompts / instructions that modify the agent’s behavior. | Choose the type that matches your use‑case: a chatbot may need episodic memory, a recommendation system may rely on semantic memory, and a tool‑driven assistant may update its procedural memory after learning a new workflow. |\n",
      "| **Background / async updates** | • Memory can be updated **on‑the‑hot‑path** (immediately before a response) or **asynchronously** via background workers. | Asynchronous updates avoid latency spikes while still keeping the knowledge base fresh. |\n",
      "| **Integration with external stores** | • Vector DBs (Chroma, Pinecone, etc.) for semantic memory. <br>• Graph databases such as **FalkorDB** for knowledge‑graph‑style memory. <br>• Cloud services (AWS Bedrock AgentCore, LangSmith) that provide managed persistence. | Plug any storage backend into the LangMem or Memory Store APIs; the agent can retrieve context from vectors, graphs, or relational tables. |\n",
      "| **Deployment & observability** | • **LangSmith** – provides auto‑scaling, checkpoint persistence, and observability for long‑running agents. <br>• One‑click deployment templates that include memory services. | When you ship an agent that runs for hours or days, LangSmith ensures the memory layer stays alive and is easy to debug. |\n",
      "\n",
      "### Putting it together – a typical workflow\n",
      "\n",
      "1. **Create the agent**  \n",
      "   ```python\n",
      "   from langchain.agents import create_tool_calling_agent\n",
      "   from langchain.memory import ConversationBufferWindowMemory\n",
      "   from langchain_core.prompts import ChatPromptTemplate\n",
      "\n",
      "   prompt = ChatPromptTemplate.from_messages([...])\n",
      "   memory = ConversationBufferWindowMemory(k=5)   # short‑term memory\n",
      "   agent = create_tool_calling_agent(llm, tools, prompt, memory=memory)\n",
      "   ```\n",
      "\n",
      "2. **Add long‑term memory** (optional)  \n",
      "   ```python\n",
      "   from langmem import LangMem, MemoryStore\n",
      "\n",
      "   mem = LangMem(store=MemoryStore(...))\n",
      "   # after each turn, extract facts and store them\n",
      "   mem.save_facts(extract_facts(response))\n",
      "   ```\n",
      "\n",
      "3. **Run the agent** – each step reads the short‑term memory, can query the long‑term store, decides on a tool, updates memory, and returns a response.\n",
      "\n",
      "4. **Persist state** – a `checkpointer` (e.g., RedisSaver) writes the whole agent state so the conversation can be resumed later or after a crash.\n",
      "\n",
      "### Key take‑aways\n",
      "\n",
      "* **Agents** are first‑class citizens in LangChain: you can compose tools, prompts, and control flow with a few lines of code.  \n",
      "* **Memory** is modular: you can start with simple buffer memory and scale to sophisticated, persistent, multi‑type stores (episodic, semantic, procedural) using LangMem and LangGraph.  \n",
      "* The ecosystem (vector DBs, graph DBs, LangSmith, Bedrock AgentCore) lets you plug the appropriate storage backend and get production‑grade reliability.  \n",
      "\n",
      "Together, these capabilities let you build agents that **remember**, **learn**, and **adapt** over short and long horizons—exactly what is needed for robust, real‑world AI assistants.\n"
     ]
    }
   ],
   "source": [
    "print(\"✔️ Answer:\\n\", response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72c43a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How does Langchain support agents and memeory?',\n",
       " 'context': [Document(id='32f856ca-998b-419f-82c7-2b5ea7875388', metadata={'source': 'mmr_rag_practice_document.txt'}, page_content='metrics, and another about hybrid retrieval connections. This increases coverage and improves the final generated answer. LangChain provides builtâ€‘in support for MMR retrieval strategies through its retriever configuration options. Developers can adjust parameters such as lambda, which controls the tradeâ€‘off between relevance and diversity. A higher lambda value prioritizes relevance, while a lower value increases diversity. Experimenting with these values is an excellent exercise for'),\n",
       "  Document(id='19b8fca0-d6c5-4961-938f-74215e509764', metadata={'source': 'mmr_rag_practice_document.txt'}, page_content='embeddings, vector databases, and reâ€‘ranking techniques. If a user asks about embeddings, pure similarity search may return five nearly identical paragraphs explaining vector space conversion. With MMR enabled, the retriever might return one paragraph about embeddings, another about vector databases, another about similarity metrics, and another about hybrid retrieval connections. This increases coverage and improves the final generated answer. LangChain provides builtâ€‘in support for MMR'),\n",
       "  Document(id='4a75170e-8cd3-4e74-a6df-4fc183d002d5', metadata={'source': 'mmr_rag_practice_document.txt'}, page_content='chunking strategies, and prompt engineering. The overlap in concepts combined with slight wording variations creates the perfect environment for testing diversification techniques. Through repeated experimentation with MMR, practitioners learn how retrieval diversity improves downstream language model performance. They also understand tradeâ€‘offs between precision and coverage. Mastering MMR leads to stronger RAG systems, better search experiences, and more intelligent AIâ€‘driven')],\n",
       " 'answer': '**How LangChain supports agents\\u202fand\\u202fmemory**\\n\\nLangChain (and its companion project\\u202fLangGraph) provides a full‑stack set of primitives that let developers build **LLM‑powered agents** that can **remember** what has happened before, update that knowledge, and reuse it later. The main pieces are:\\n\\n| Area | What LangChain offers | How it is used |\\n|------|----------------------|----------------|\\n| **Agent framework** | • `AgentExecutor`, `create_agent`, `ZeroShotAgent`, tool‑calling agents, and the newer **Agentic\\u202fGraph** API (via LangGraph). <br>• Built‑in support for tool integration, planning, and multi‑step reasoning. | You define a set of tools (search, DB lookup, code execution, etc.) and a prompt template; the executor orchestrates calls to the LLM, decides which tool to invoke, and stitches the results together. |\\n| **Short‑term (thread‑level) memory** | • `ConversationBufferMemory`, `ConversationBufferWindowMemory`, `ConversationSummaryMemory`, etc. <br>• Memory is attached to an agent’s `state` and persisted with a **checkpointer** (e.g., `InMemorySaver`, Redis, SQLite). | The memory stores the most recent turns of a conversation so the LLM can see context without re‑sending the whole history. |\\n| **Long‑term / persistent memory** | • **LangMem SDK** – a library that extracts salient facts from interactions, writes them to any backing store, and can later retrieve them as context. <br>• **Memory Store** (part of LangGraph) – a dedicated service for episodic, semantic, and procedural memories that runs in the background. | Use `LangMem` to create “facts” (e.g., user preferences, task outcomes) that survive across sessions. The Memory Store can be queried by the agent to recall past episodes or to update procedural knowledge (e.g., new rules). |\\n| **Memory types** | • **Episodic memory** – stores specific interactions (who said what, when). <br>• **Semantic memory** – stores extracted facts or embeddings for fast similarity search. <br>• **Procedural memory** – stores updated prompts / instructions that modify the agent’s behavior. | Choose the type that matches your use‑case: a chatbot may need episodic memory, a recommendation system may rely on semantic memory, and a tool‑driven assistant may update its procedural memory after learning a new workflow. |\\n| **Background / async updates** | • Memory can be updated **on‑the‑hot‑path** (immediately before a response) or **asynchronously** via background workers. | Asynchronous updates avoid latency spikes while still keeping the knowledge base fresh. |\\n| **Integration with external stores** | • Vector DBs (Chroma, Pinecone, etc.) for semantic memory. <br>• Graph databases such as **FalkorDB** for knowledge‑graph‑style memory. <br>• Cloud services (AWS Bedrock AgentCore, LangSmith) that provide managed persistence. | Plug any storage backend into the LangMem or Memory Store APIs; the agent can retrieve context from vectors, graphs, or relational tables. |\\n| **Deployment & observability** | • **LangSmith** – provides auto‑scaling, checkpoint persistence, and observability for long‑running agents. <br>• One‑click deployment templates that include memory services. | When you ship an agent that runs for hours or days, LangSmith ensures the memory layer stays alive and is easy to debug. |\\n\\n### Putting it together – a typical workflow\\n\\n1. **Create the agent**  \\n   ```python\\n   from langchain.agents import create_tool_calling_agent\\n   from langchain.memory import ConversationBufferWindowMemory\\n   from langchain_core.prompts import ChatPromptTemplate\\n\\n   prompt = ChatPromptTemplate.from_messages([...])\\n   memory = ConversationBufferWindowMemory(k=5)   # short‑term memory\\n   agent = create_tool_calling_agent(llm, tools, prompt, memory=memory)\\n   ```\\n\\n2. **Add long‑term memory** (optional)  \\n   ```python\\n   from langmem import LangMem, MemoryStore\\n\\n   mem = LangMem(store=MemoryStore(...))\\n   # after each turn, extract facts and store them\\n   mem.save_facts(extract_facts(response))\\n   ```\\n\\n3. **Run the agent** – each step reads the short‑term memory, can query the long‑term store, decides on a tool, updates memory, and returns a response.\\n\\n4. **Persist state** – a `checkpointer` (e.g., RedisSaver) writes the whole agent state so the conversation can be resumed later or after a crash.\\n\\n### Key take‑aways\\n\\n* **Agents** are first‑class citizens in LangChain: you can compose tools, prompts, and control flow with a few lines of code.  \\n* **Memory** is modular: you can start with simple buffer memory and scale to sophisticated, persistent, multi‑type stores (episodic, semantic, procedural) using LangMem and LangGraph.  \\n* The ecosystem (vector DBs, graph DBs, LangSmith, Bedrock AgentCore) lets you plug the appropriate storage backend and get production‑grade reliability.  \\n\\nTogether, these capabilities let you build agents that **remember**, **learn**, and **adapt** over short and long horizons—exactly what is needed for robust, real‑world AI assistants.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_UDEMY (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
