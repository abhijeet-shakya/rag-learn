{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3df1b4ec",
   "metadata": {},
   "source": [
    "### Reranking Hybrid Search Statergies\n",
    "\n",
    "Re-ranking is a second stage filtering process in retrival system , especially in RAG pipelines, where we:\n",
    "1. First use a retriever(like BM25, FAISS, HYBRID) to fetch top-k documents quickly.\n",
    "2. Then use a more accurate but slower model(like a cross-encoder or LLM) to res-score and reorder those documents by relevance to the query.\n",
    "\n",
    "üëâüèø It ensures that the most relevant documents appear at the top, improve the final answer from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00993629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.document_loaders import TextLoader\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "from langchain_classic.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15609cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain_sample.txt'}, page_content='In modern information retrieval systems, understanding the difference between sparse and dense retrievers is essential. Sparse retrieval methods are based on exact or near-exact term matching. Techniques such as TF-IDF and BM25 fall into this category. These methods represent documents as sparse vectors where each dimension corresponds to a word or token in the vocabulary. If a word does not appear in a document, its value is zero. Sparse retrievers are highly interpretable and work'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='retrievers are highly interpretable and work exceptionally well when queries contain exact keywords that also appear in documents. BM25 improves upon earlier sparse methods by incorporating term frequency saturation and document length normalization. This makes BM25 more robust across documents of varying sizes. In practice, sparse retrievers are very effective for technical content, error messages, product names, identifiers, and situations where wording consistency is high. However, sparse'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='wording consistency is high. However, sparse methods struggle when users phrase questions differently from how information is written. Dense retrieval addresses this limitation by focusing on semantic similarity rather than exact word overlap. In dense retrieval, documents and queries are converted into embeddings. Embeddings are dense numerical vectors produced by neural models that capture meaning rather than surface-level text. Two texts that convey similar ideas will have embeddings that'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='convey similar ideas will have embeddings that are close together in vector space, even if they share few or no common words. Embedding models such as sentence transformers are commonly used for dense retrieval. These models are trained so that semantically related sentences cluster together. Once embeddings are generated, they are stored in vector databases like FAISS. When a query arrives, its embedding is compared against stored vectors using similarity metrics such as cosine similarity or'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='similarity metrics such as cosine similarity or inner product. Dense retrievers excel at handling paraphrased queries, natural language questions, and conceptual search. However, they can sometimes retrieve documents that are semantically related but miss critical exact details. This is why dense retrieval alone may not always be sufficient. Hybrid retrieval combines sparse and dense retrievers to leverage the strengths of both approaches. In a hybrid system, documents may be retrieved'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='In a hybrid system, documents may be retrieved independently by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='documents. These documents are then passed as context to the language model. This context is critical in Retrieval-Augmented Generation workflows, as it grounds the model√¢‚Ç¨‚Ñ¢s responses in external knowledge. Re-ranking is an additional step that significantly improves retrieval quality. In re-ranking, an initial set of documents is retrieved using a fast method such as BM25 or vector similarity search. A more expensive but accurate model, often a cross-encoder, then re-scores each document with'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='cross-encoder, then re-scores each document with respect to the query. This allows the system to reorder results based on deeper query-document interaction. Re-ranking models evaluate the query and document together, rather than independently as in embedding-based retrieval. This leads to more precise relevance judgments. Re-ranking is especially useful when the top retrieved documents are closely related and subtle differences matter. Practicing re-ranking requires datasets where documents'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='re-ranking requires datasets where documents share overlapping themes but differ in relevance. By adjusting retrieval strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval evaluation. Developers can measure recall from sparse retrieval, semantic coverage from dense retrieval, and precision improvements from re-ranking. By iterating on these'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='from re-ranking. By iterating on these components, one can build highly effective retrieval systems. Understanding these concepts deeply is critical for building production-grade RAG systems. Sparse retrieval ensures exactness, dense retrieval ensures semantic understanding, and re-ranking ensures precision. Together, they form the foundation of modern intelligent search and question-answering systems. In modern information retrieval systems, understanding the difference between sparse and'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='understanding the difference between sparse and dense retrievers is essential. Sparse retrieval methods are based on exact or near-exact term matching. Techniques such as TF-IDF and BM25 fall into this category. These methods represent documents as sparse vectors where each dimension corresponds to a word or token in the vocabulary. If a word does not appear in a document, its value is zero. Sparse retrievers are highly interpretable and work exceptionally well when queries contain exact'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='exceptionally well when queries contain exact keywords that also appear in documents. BM25 improves upon earlier sparse methods by incorporating term frequency saturation and document length normalization. This makes BM25 more robust across documents of varying sizes. In practice, sparse retrievers are very effective for technical content, error messages, product names, identifiers, and situations where wording consistency is high. However, sparse methods struggle when users phrase questions'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='methods struggle when users phrase questions differently from how information is written. Dense retrieval addresses this limitation by focusing on semantic similarity rather than exact word overlap. In dense retrieval, documents and queries are converted into embeddings. Embeddings are dense numerical vectors produced by neural models that capture meaning rather than surface-level text. Two texts that convey similar ideas will have embeddings that are close together in vector space, even if'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='that are close together in vector space, even if they share few or no common words. Embedding models such as sentence transformers are commonly used for dense retrieval. These models are trained so that semantically related sentences cluster together. Once embeddings are generated, they are stored in vector databases like FAISS. When a query arrives, its embedding is compared against stored vectors using similarity metrics such as cosine similarity or inner product. Dense retrievers excel at'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='or inner product. Dense retrievers excel at handling paraphrased queries, natural language questions, and conceptual search. However, they can sometimes retrieve documents that are semantically related but miss critical exact details. This is why dense retrieval alone may not always be sufficient. Hybrid retrieval combines sparse and dense retrievers to leverage the strengths of both approaches. In a hybrid system, documents may be retrieved independently by a sparse retriever like BM25 and a'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='the language model. This context is critical in Retrieval-Augmented Generation workflows, as it grounds the model√¢‚Ç¨‚Ñ¢s responses in external knowledge. Re-ranking is an additional step that significantly improves retrieval quality. In re-ranking, an initial set of documents is retrieved using a fast method such as BM25 or vector similarity search. A more expensive but accurate model, often a cross-encoder, then re-scores each document with respect to the query. This allows the system to reorder'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='to the query. This allows the system to reorder results based on deeper query-document interaction. Re-ranking models evaluate the query and document together, rather than independently as in embedding-based retrieval. This leads to more precise relevance judgments. Re-ranking is especially useful when the top retrieved documents are closely related and subtle differences matter. Practicing re-ranking requires datasets where documents share overlapping themes but differ in relevance. By'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='overlapping themes but differ in relevance. By adjusting retrieval strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval,'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval evaluation. Developers can measure recall from sparse retrieval, semantic coverage from dense retrieval, and precision improvements from re-ranking. By iterating on these components, one can build highly effective retrieval systems.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='one can build highly effective retrieval systems. Understanding these concepts deeply is critical for building production-grade RAG systems. Sparse retrieval ensures exactness, dense retrieval ensures semantic understanding, and re-ranking ensures precision. Together, they form the foundation of modern intelligent search and question-answering systems. In modern information retrieval systems, understanding the difference between sparse and dense retrievers is essential. Sparse retrieval methods'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='retrievers is essential. Sparse retrieval methods are based on exact or near-exact term matching. Techniques such as TF-IDF and BM25 fall into this category. These methods represent documents as sparse vectors where each dimension corresponds to a word or token in the vocabulary. If a word does not appear in a document, its value is zero. Sparse retrievers are highly interpretable and work exceptionally well when queries contain exact keywords that also appear in documents. BM25 improves upon'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='that also appear in documents. BM25 improves upon earlier sparse methods by incorporating term frequency saturation and document length normalization. This makes BM25 more robust across documents of varying sizes. In practice, sparse retrievers are very effective for technical content, error messages, product names, identifiers, and situations where wording consistency is high. However, sparse methods struggle when users phrase questions differently from how information is written. Dense'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='from how information is written. Dense retrieval addresses this limitation by focusing on semantic similarity rather than exact word overlap. In dense retrieval, documents and queries are converted into embeddings. Embeddings are dense numerical vectors produced by neural models that capture meaning rather than surface-level text. Two texts that convey similar ideas will have embeddings that are close together in vector space, even if they share few or no common words. Embedding models such as'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='few or no common words. Embedding models such as sentence transformers are commonly used for dense retrieval. These models are trained so that semantically related sentences cluster together. Once embeddings are generated, they are stored in vector databases like FAISS. When a query arrives, its embedding is compared against stored vectors using similarity metrics such as cosine similarity or inner product. Dense retrievers excel at handling paraphrased queries, natural language questions, and'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='queries, natural language questions, and conceptual search. However, they can sometimes retrieve documents that are semantically related but miss critical exact details. This is why dense retrieval alone may not always be sufficient. Hybrid retrieval combines sparse and dense retrievers to leverage the strengths of both approaches. In a hybrid system, documents may be retrieved independently by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in Retrieval-Augmented Generation workflows, as it grounds the'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='Generation workflows, as it grounds the model√¢‚Ç¨‚Ñ¢s responses in external knowledge. Re-ranking is an additional step that significantly improves retrieval quality. In re-ranking, an initial set of documents is retrieved using a fast method such as BM25 or vector similarity search. A more expensive but accurate model, often a cross-encoder, then re-scores each document with respect to the query. This allows the system to reorder results based on deeper query-document interaction. Re-ranking'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='on deeper query-document interaction. Re-ranking models evaluate the query and document together, rather than independently as in embedding-based retrieval. This leads to more precise relevance judgments. Re-ranking is especially useful when the top retrieved documents are closely related and subtle differences matter. Practicing re-ranking requires datasets where documents share overlapping themes but differ in relevance. By adjusting retrieval strategies, embedding models, and re-rankers,'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval evaluation. Developers can measure recall from sparse retrieval, semantic coverage from dense retrieval, and precision improvements from re-ranking. By iterating on these components, one can build highly effective retrieval systems. Understanding these concepts deeply is critical for building'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='these concepts deeply is critical for building production-grade RAG systems. Sparse retrieval ensures exactness, dense retrieval ensures semantic understanding, and re-ranking ensures precision. Together, they form the foundation of modern intelligent search and question-answering systems. In modern information retrieval systems, understanding the difference between sparse and dense retrievers is essential. Sparse retrieval methods are based on exact or near-exact term matching. Techniques such'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='or near-exact term matching. Techniques such as TF-IDF and BM25 fall into this category. These methods represent documents as sparse vectors where each dimension corresponds to a word or token in the vocabulary. If a word does not appear in a document, its value is zero. Sparse retrievers are highly interpretable and work exceptionally well when queries contain exact keywords that also appear in documents. BM25 improves upon earlier sparse methods by incorporating term frequency saturation and'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='by incorporating term frequency saturation and document length normalization. This makes BM25 more robust across documents of varying sizes. In practice, sparse retrievers are very effective for technical content, error messages, product names, identifiers, and situations where wording consistency is high. However, sparse methods struggle when users phrase questions differently from how information is written. Dense retrieval addresses this limitation by focusing on semantic similarity rather'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='by focusing on semantic similarity rather than exact word overlap. In dense retrieval, documents and queries are converted into embeddings. Embeddings are dense numerical vectors produced by neural models that capture meaning rather than surface-level text. Two texts that convey similar ideas will have embeddings that are close together in vector space, even if they share few or no common words. Embedding models such as sentence transformers are commonly used for dense retrieval. These models'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='commonly used for dense retrieval. These models are trained so that semantically related sentences cluster together. Once embeddings are generated, they are stored in vector databases like FAISS. When a query arrives, its embedding is compared against stored vectors using similarity metrics such as cosine similarity or inner product. Dense retrievers excel at handling paraphrased queries, natural language questions, and conceptual search. However, they can sometimes retrieve documents that are'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='they can sometimes retrieve documents that are semantically related but miss critical exact details. This is why dense retrieval alone may not always be sufficient. Hybrid retrieval combines sparse and dense retrievers to leverage the strengths of both approaches. In a hybrid system, documents may be retrieved independently by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in Retrieval-Augmented Generation workflows, as it grounds the model√¢‚Ç¨‚Ñ¢s responses in external knowledge. Re-ranking is an additional step'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='knowledge. Re-ranking is an additional step that significantly improves retrieval quality. In re-ranking, an initial set of documents is retrieved using a fast method such as BM25 or vector similarity search. A more expensive but accurate model, often a cross-encoder, then re-scores each document with respect to the query. This allows the system to reorder results based on deeper query-document interaction. Re-ranking models evaluate the query and document together, rather than independently as'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='document together, rather than independently as in embedding-based retrieval. This leads to more precise relevance judgments. Re-ranking is especially useful when the top retrieved documents are closely related and subtle differences matter. Practicing re-ranking requires datasets where documents share overlapping themes but differ in relevance. By adjusting retrieval strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='they are sent to the language model as context. This process creates an excellent environment for practicing retrieval evaluation. Developers can measure recall from sparse retrieval, semantic coverage from dense retrieval, and precision improvements from re-ranking. By iterating on these components, one can build highly effective retrieval systems. Understanding these concepts deeply is critical for building production-grade RAG systems. Sparse retrieval ensures exactness, dense retrieval'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='retrieval ensures exactness, dense retrieval ensures semantic understanding, and re-ranking ensures precision. Together, they form the foundation of modern intelligent search and question-answering systems. In modern information retrieval systems, understanding the difference between sparse and dense retrievers is essential. Sparse retrieval methods are based on exact or near-exact term matching. Techniques such as TF-IDF and BM25 fall into this category. These methods represent documents as'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='category. These methods represent documents as sparse vectors where each dimension corresponds to a word or token in the vocabulary. If a word does not appear in a document, its value is zero. Sparse retrievers are highly interpretable and work exceptionally well when queries contain exact keywords that also appear in documents. BM25 improves upon earlier sparse methods by incorporating term frequency saturation and document length normalization. This makes BM25 more robust across documents of'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='This makes BM25 more robust across documents of varying sizes. In practice, sparse retrievers are very effective for technical content, error messages, product names, identifiers, and situations where wording consistency is high. However, sparse methods struggle when users phrase questions differently from how information is written. Dense retrieval addresses this limitation by focusing on semantic similarity rather than exact word overlap. In dense retrieval, documents and queries are'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='In dense retrieval, documents and queries are converted into embeddings. Embeddings are dense numerical vectors produced by neural models that capture meaning rather than surface-level text. Two texts that convey similar ideas will have embeddings that are close together in vector space, even if they share few or no common words. Embedding models such as sentence transformers are commonly used for dense retrieval. These models are trained so that semantically related sentences cluster together.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='semantically related sentences cluster together. Once embeddings are generated, they are stored in vector databases like FAISS. When a query arrives, its embedding is compared against stored vectors using similarity metrics such as cosine similarity or inner product. Dense retrievers excel at handling paraphrased queries, natural language questions, and conceptual search. However, they can sometimes retrieve documents that are semantically related but miss critical exact details. This is why'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='but miss critical exact details. This is why dense retrieval alone may not always be sufficient. Hybrid retrieval combines sparse and dense retrievers to leverage the strengths of both approaches. In a hybrid system, documents may be retrieved independently by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in Retrieval-Augmented Generation workflows, as it grounds the model√¢‚Ç¨‚Ñ¢s responses in external knowledge. Re-ranking is an additional step that significantly improves retrieval quality. In re-ranking, an initial set'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='retrieval quality. In re-ranking, an initial set of documents is retrieved using a fast method such as BM25 or vector similarity search. A more expensive but accurate model, often a cross-encoder, then re-scores each document with respect to the query. This allows the system to reorder results based on deeper query-document interaction. Re-ranking models evaluate the query and document together, rather than independently as in embedding-based retrieval. This leads to more precise relevance'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='retrieval. This leads to more precise relevance judgments. Re-ranking is especially useful when the top retrieved documents are closely related and subtle differences matter. Practicing re-ranking requires datasets where documents share overlapping themes but differ in relevance. By adjusting retrieval strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='an excellent environment for practicing retrieval evaluation. Developers can measure recall from sparse retrieval, semantic coverage from dense retrieval, and precision improvements from re-ranking. By iterating on these components, one can build highly effective retrieval systems. Understanding these concepts deeply is critical for building production-grade RAG systems. Sparse retrieval ensures exactness, dense retrieval ensures semantic understanding, and re-ranking ensures precision.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='understanding, and re-ranking ensures precision. Together, they form the foundation of modern intelligent search and question-answering systems. In modern information retrieval systems, understanding the difference between sparse and dense retrievers is essential. Sparse retrieval methods are based on exact or near-exact term matching. Techniques such as TF-IDF and BM25 fall into this category. These methods represent documents as sparse vectors where each dimension corresponds to a word or'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='where each dimension corresponds to a word or token in the vocabulary. If a word does not appear in a document, its value is zero. Sparse retrievers are highly interpretable and work exceptionally well when queries contain exact keywords that also appear in documents. BM25 improves upon earlier sparse methods by incorporating term frequency saturation and document length normalization. This makes BM25 more robust across documents of varying sizes. In practice, sparse retrievers are very'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='sizes. In practice, sparse retrievers are very effective for technical content, error messages, product names, identifiers, and situations where wording consistency is high. However, sparse methods struggle when users phrase questions differently from how information is written. Dense retrieval addresses this limitation by focusing on semantic similarity rather than exact word overlap. In dense retrieval, documents and queries are converted into embeddings. Embeddings are dense numerical'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='into embeddings. Embeddings are dense numerical vectors produced by neural models that capture meaning rather than surface-level text. Two texts that convey similar ideas will have embeddings that are close together in vector space, even if they share few or no common words. Embedding models such as sentence transformers are commonly used for dense retrieval. These models are trained so that semantically related sentences cluster together. Once embeddings are generated, they are stored in'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='Once embeddings are generated, they are stored in vector databases like FAISS. When a query arrives, its embedding is compared against stored vectors using similarity metrics such as cosine similarity or inner product. Dense retrievers excel at handling paraphrased queries, natural language questions, and conceptual search. However, they can sometimes retrieve documents that are semantically related but miss critical exact details. This is why dense retrieval alone may not always be sufficient.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='retrieval alone may not always be sufficient. Hybrid retrieval combines sparse and dense retrievers to leverage the strengths of both approaches. In a hybrid system, documents may be retrieved independently by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in Retrieval-Augmented Generation workflows, as it grounds the model√¢‚Ç¨‚Ñ¢s responses in external knowledge. Re-ranking is an additional step that significantly improves retrieval quality. In re-ranking, an initial set of documents is retrieved using a'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='an initial set of documents is retrieved using a fast method such as BM25 or vector similarity search. A more expensive but accurate model, often a cross-encoder, then re-scores each document with respect to the query. This allows the system to reorder results based on deeper query-document interaction. Re-ranking models evaluate the query and document together, rather than independently as in embedding-based retrieval. This leads to more precise relevance judgments. Re-ranking is especially'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='relevance judgments. Re-ranking is especially useful when the top retrieved documents are closely related and subtle differences matter. Practicing re-ranking requires datasets where documents share overlapping themes but differ in relevance. By adjusting retrieval strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval evaluation. Developers can measure recall'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='evaluation. Developers can measure recall from sparse retrieval, semantic coverage from dense retrieval, and precision improvements from re-ranking. By iterating on these components, one can build highly effective retrieval systems. Understanding these concepts deeply is critical for building production-grade RAG systems. Sparse retrieval ensures exactness, dense retrieval ensures semantic understanding, and re-ranking ensures precision. Together, they form the foundation of modern intelligent'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='they form the foundation of modern intelligent search and question-answering systems. In modern information retrieval systems, understanding the difference between sparse and dense retrievers is essential. Sparse retrieval methods are based on exact or near-exact term matching. Techniques such as TF-IDF and BM25 fall into this category. These methods represent documents as sparse vectors where each dimension corresponds to a word or token in the vocabulary. If a word does not appear in a'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='in the vocabulary. If a word does not appear in a document, its value is zero. Sparse retrievers are highly interpretable and work exceptionally well when queries contain exact keywords that also appear in documents. BM25 improves upon earlier sparse methods by incorporating term frequency saturation and document length normalization. This makes BM25 more robust across documents of varying sizes. In practice, sparse retrievers are very effective for technical content, error messages, product'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='for technical content, error messages, product names, identifiers, and situations where wording consistency is high. However, sparse methods struggle when users phrase questions differently from how information is written. Dense retrieval addresses this limitation by focusing on semantic similarity rather than exact word overlap. In dense retrieval, documents and queries are converted into embeddings. Embeddings are dense numerical vectors produced by neural models that capture meaning rather'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='by neural models that capture meaning rather than surface-level text. Two texts that convey similar ideas will have embeddings that are close together in vector space, even if they share few or no common words. Embedding models such as sentence transformers are commonly used for dense retrieval. These models are trained so that semantically related sentences cluster together. Once embeddings are generated, they are stored in vector databases like FAISS. When a query arrives, its embedding is'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='FAISS. When a query arrives, its embedding is compared against stored vectors using similarity metrics such as cosine similarity or inner product. Dense retrievers excel at handling paraphrased queries, natural language questions, and conceptual search. However, they can sometimes retrieve documents that are semantically related but miss critical exact details. This is why dense retrieval alone may not always be sufficient. Hybrid retrieval combines sparse and dense retrievers to leverage the'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='sparse and dense retrievers to leverage the strengths of both approaches. In a hybrid system, documents may be retrieved independently by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in Retrieval-Augmented Generation workflows, as it grounds the model√¢‚Ç¨‚Ñ¢s responses in external knowledge. Re-ranking is an additional step that significantly improves retrieval quality. In re-ranking, an initial set of documents is retrieved using a fast method such as BM25 or vector similarity search. A more expensive but'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='or vector similarity search. A more expensive but accurate model, often a cross-encoder, then re-scores each document with respect to the query. This allows the system to reorder results based on deeper query-document interaction. Re-ranking models evaluate the query and document together, rather than independently as in embedding-based retrieval. This leads to more precise relevance judgments. Re-ranking is especially useful when the top retrieved documents are closely related and subtle'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='documents are closely related and subtle differences matter. Practicing re-ranking requires datasets where documents share overlapping themes but differ in relevance. By adjusting retrieval strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval evaluation. Developers can measure recall from sparse retrieval, semantic coverage from dense retrieval, and'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='semantic coverage from dense retrieval, and precision improvements from re-ranking. By iterating on these components, one can build highly effective retrieval systems. Understanding these concepts deeply is critical for building production-grade RAG systems. Sparse retrieval ensures exactness, dense retrieval ensures semantic understanding, and re-ranking ensures precision. Together, they form the foundation of modern intelligent search and question-answering systems. In modern information'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='question-answering systems. In modern information retrieval systems, understanding the difference between sparse and dense retrievers is essential. Sparse retrieval methods are based on exact or near-exact term matching. Techniques such as TF-IDF and BM25 fall into this category. These methods represent documents as sparse vectors where each dimension corresponds to a word or token in the vocabulary. If a word does not appear in a document, its value is zero. Sparse retrievers are highly'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='its value is zero. Sparse retrievers are highly interpretable and work exceptionally well when queries contain exact keywords that also appear in documents. BM25 improves upon earlier sparse methods by incorporating term frequency saturation and document length normalization. This makes BM25 more robust across documents of varying sizes. In practice, sparse retrievers are very effective for technical content, error messages, product names, identifiers, and situations where wording consistency'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='and situations where wording consistency is high. However, sparse methods struggle when users phrase questions differently from how information is written. Dense retrieval addresses this limitation by focusing on semantic similarity rather than exact word overlap. In dense retrieval, documents and queries are converted into embeddings. Embeddings are dense numerical vectors produced by neural models that capture')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the text file\n",
    "loader = TextLoader(\n",
    "    \"langchain_sample.txt\"\n",
    ")\n",
    "raw_docs = loader.load()\n",
    "\n",
    "# Split the text into documents chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5d0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## user query\n",
    "query = \"How can i use langchain to build application with memory and tools?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "304fcbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAISS  and Huggingface model embeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_documents(docs, embedding_model)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\":8})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec197e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000025DA1911910>, search_kwargs={'k': 8})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ff7187a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x0000025DA345B1D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000025DA34BF860>, model_name='openai/gpt-oss-20b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## prompt and use the llm\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model='groq:openai/gpt-oss-20b'\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c29775b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are  a helpful assistant. Your task is to rank the following documents from most to least relevant to the user's question.\n",
    "User Question: \"{question}\"\n",
    "Documents:{documents}\n",
    "\n",
    "Instruction: \n",
    "- Think about the relevance of each document to the user's question.\n",
    "- Return the list of document indices in ranked order, starting from the most relevant.\n",
    "\n",
    "Output format : comma-separated document indices (e.g,2,1,3,0,..)                                                                                                                                                                                            \n",
    "\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c1d891f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f15a1ffd-670e-4fbb-8863-76ca0c11cd43', metadata={'source': 'langchain_sample.txt'}, page_content='ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context.'),\n",
       " Document(id='05abae1e-f581-443b-9a7c-1d6ea247f6ef', metadata={'source': 'langchain_sample.txt'}, page_content='using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in Retrieval-Augmented Generation workflows, as it grounds the'),\n",
       " Document(id='a8b9687c-01f8-401c-8cbd-6c6eec1b7be4', metadata={'source': 'langchain_sample.txt'}, page_content='by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in'),\n",
       " Document(id='e1c57ff5-0fe0-4d2e-81c1-6f2d711baec6', metadata={'source': 'langchain_sample.txt'}, page_content='In a hybrid system, documents may be retrieved independently by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as'),\n",
       " Document(id='b37f05a0-9680-4261-afa5-54e1ab387946', metadata={'source': 'langchain_sample.txt'}, page_content='strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the'),\n",
       " Document(id='de480a78-96bd-4b80-93fd-83e9f4c99eb2', metadata={'source': 'langchain_sample.txt'}, page_content='it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval'),\n",
       " Document(id='43b07c70-e0d3-4c53-b608-48f84a13914a', metadata={'source': 'langchain_sample.txt'}, page_content='and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval evaluation. Developers can measure recall'),\n",
       " Document(id='ef913d65-ae20-463c-9779-9b0c8acd978c', metadata={'source': 'langchain_sample.txt'}, page_content='overlapping themes but differ in relevance. By adjusting retrieval strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval,')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_docs = retriever.invoke(query)\n",
    "retrieve_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "995dabbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['documents', 'question'], input_types={}, partial_variables={}, template='\\nYou are  a helpful assistant. Your task is to rank the following documents from most to least relevant to the user\\'s question.\\nUser Question: \"{question}\"\\nDocuments:{documents}\\n\\nInstruction: \\n- Think about the relevance of each document to the user\\'s question.\\n- Return the list of document indices in ranked order, starting from the most relevant.\\n\\nOutput format : comma-separated document indices (e.g,2,1,3,0,..)                                                                                                                                                                                            \\n\\n')\n",
       "| ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x0000025DA345B1D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000025DA34BF860>, model_name='openai/gpt-oss-20b', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "220db4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lines = [f\"{i+1}. {doc.page_content}\" for i, doc in enumerate(retrieve_docs)]\n",
    "formatted_docs = \"\\n\".join(doc_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87058497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context.',\n",
       " '2. using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in Retrieval-Augmented Generation workflows, as it grounds the',\n",
       " '3. by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in',\n",
       " '4. In a hybrid system, documents may be retrieved independently by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as',\n",
       " '5. strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the',\n",
       " '6. it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval',\n",
       " '7. and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval evaluation. Developers can measure recall',\n",
       " '8. overlapping themes but differ in relevance. By adjusting retrieval strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval,']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3caea929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context.\\n2. using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in Retrieval-Augmented Generation workflows, as it grounds the\\n3. by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in\\n4. In a hybrid system, documents may be retrieved independently by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as\\n5. strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the\\n6. it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval\\n7. and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval evaluation. Developers can measure recall\\n8. overlapping themes but differ in relevance. By adjusting retrieval strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval,'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01301b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,5,6,7,2,3,4,8'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": query, \"documents\":formatted_docs})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "716d7c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 5, 6, 1, 2, 3, 7]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Parse and rerank\n",
    "indices = [int(x.strip())-1 for x in response.split(\",\") if x.strip().isdigit()]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39b73c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f15a1ffd-670e-4fbb-8863-76ca0c11cd43', metadata={'source': 'langchain_sample.txt'}, page_content='ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context.'),\n",
       " Document(id='05abae1e-f581-443b-9a7c-1d6ea247f6ef', metadata={'source': 'langchain_sample.txt'}, page_content='using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in Retrieval-Augmented Generation workflows, as it grounds the'),\n",
       " Document(id='a8b9687c-01f8-401c-8cbd-6c6eec1b7be4', metadata={'source': 'langchain_sample.txt'}, page_content='by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in'),\n",
       " Document(id='e1c57ff5-0fe0-4d2e-81c1-6f2d711baec6', metadata={'source': 'langchain_sample.txt'}, page_content='In a hybrid system, documents may be retrieved independently by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as'),\n",
       " Document(id='b37f05a0-9680-4261-afa5-54e1ab387946', metadata={'source': 'langchain_sample.txt'}, page_content='strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the'),\n",
       " Document(id='de480a78-96bd-4b80-93fd-83e9f4c99eb2', metadata={'source': 'langchain_sample.txt'}, page_content='it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval'),\n",
       " Document(id='43b07c70-e0d3-4c53-b608-48f84a13914a', metadata={'source': 'langchain_sample.txt'}, page_content='and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval evaluation. Developers can measure recall'),\n",
       " Document(id='ef913d65-ae20-463c-9779-9b0c8acd978c', metadata={'source': 'langchain_sample.txt'}, page_content='overlapping themes but differ in relevance. By adjusting retrieval strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval,')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4acee7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f15a1ffd-670e-4fbb-8863-76ca0c11cd43', metadata={'source': 'langchain_sample.txt'}, page_content='ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context.'),\n",
       " Document(id='b37f05a0-9680-4261-afa5-54e1ab387946', metadata={'source': 'langchain_sample.txt'}, page_content='strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the'),\n",
       " Document(id='de480a78-96bd-4b80-93fd-83e9f4c99eb2', metadata={'source': 'langchain_sample.txt'}, page_content='it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval'),\n",
       " Document(id='43b07c70-e0d3-4c53-b608-48f84a13914a', metadata={'source': 'langchain_sample.txt'}, page_content='and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval evaluation. Developers can measure recall'),\n",
       " Document(id='05abae1e-f581-443b-9a7c-1d6ea247f6ef', metadata={'source': 'langchain_sample.txt'}, page_content='using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in Retrieval-Augmented Generation workflows, as it grounds the'),\n",
       " Document(id='a8b9687c-01f8-401c-8cbd-6c6eec1b7be4', metadata={'source': 'langchain_sample.txt'}, page_content='by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in'),\n",
       " Document(id='e1c57ff5-0fe0-4d2e-81c1-6f2d711baec6', metadata={'source': 'langchain_sample.txt'}, page_content='In a hybrid system, documents may be retrieved independently by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as'),\n",
       " Document(id='ef913d65-ae20-463c-9779-9b0c8acd978c', metadata={'source': 'langchain_sample.txt'}, page_content='overlapping themes but differ in relevance. By adjusting retrieval strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval,')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_docs = [retrieve_docs[i] for i in indices if 0<=i <len(retrieve_docs)]\n",
    "reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41b78f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Reranked Results:\n",
      "\n",
      "Rank: 1: \n",
      "ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context.\n",
      "\n",
      "Rank: 2: \n",
      "strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the\n",
      "\n",
      "Rank: 3: \n",
      "it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval\n",
      "\n",
      "Rank: 4: \n",
      "and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval, dense retrieval, or a hybrid approach. A re-ranker then refines the order of these chunks before they are sent to the language model as context. This process creates an excellent environment for practicing retrieval evaluation. Developers can measure recall\n",
      "\n",
      "Rank: 5: \n",
      "using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in Retrieval-Augmented Generation workflows, as it grounds the\n",
      "\n",
      "Rank: 6: \n",
      "by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as context to the language model. This context is critical in\n",
      "\n",
      "Rank: 7: \n",
      "In a hybrid system, documents may be retrieved independently by a sparse retriever like BM25 and a dense retriever using embeddings. The results are then merged or scored together. LangChain provides abstractions that make building hybrid retrievers straightforward, allowing developers to experiment with different weighting strategies. Within LangChain, retrievers are treated as modular components. A retriever takes a query and returns relevant documents. These documents are then passed as\n",
      "\n",
      "Rank: 8: \n",
      "overlapping themes but differ in relevance. By adjusting retrieval strategies, embedding models, and re-rankers, developers can observe how result ordering changes. LangChain supports re-ranking integrations, making it a suitable framework for experimentation and learning. In a typical LangChain RAG pipeline, documents are first loaded and split into chunks. Each chunk is embedded and stored. During query time, a retriever fetches candidate chunks. These chunks may come from sparse retrieval,\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Show results\n",
    "print(\"\\nüìä Final Reranked Results:\")\n",
    "for i, doc in enumerate(reranked_docs,1):\n",
    "    print(f\"\\nRank: {i}: \\n{doc.page_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_UDEMY (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
