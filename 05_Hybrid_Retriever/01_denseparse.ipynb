{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0768800e",
   "metadata": {},
   "source": [
    "# Hybrid Retriever - Combining Dense And Sparse Retriever\n",
    "\n",
    "## Hybrid Retrieval: What It Is and Why It Matters\n",
    "\n",
    "### What Is Hybrid Retrieval?\n",
    "\n",
    "**Hybrid retrieval** is an information retrieval approach that combines:\n",
    "\n",
    "- **Sparse (lexical) search** ‚Äî e.g. BM25  \n",
    "- **Dense (semantic) search** ‚Äî vector embeddings\n",
    "\n",
    "The goal is to leverage the strengths of both methods to retrieve more relevant documents than either approach alone.\n",
    "\n",
    "In practice, hybrid retrieval runs **both searches in parallel** and then **combines or re-ranks** the results using weighted scores or fusion techniques.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Not Use Just One Method?\n",
    "\n",
    "#### Sparse (BM25) Only\n",
    "‚úÖ Great for exact keyword matches  \n",
    "‚ùå Weak with synonyms, paraphrasing, and natural language queries  \n",
    "\n",
    "#### Dense (Semantic) Only\n",
    "‚úÖ Understands meaning and intent  \n",
    "‚ùå Can miss rare or critical keywords  \n",
    "‚ùå Less transparent scoring  \n",
    "\n",
    "Hybrid retrieval solves these limitations by using **both signals together**.\n",
    "\n",
    "---\n",
    "\n",
    "### Benefits of Hybrid Retrieval\n",
    "\n",
    "#### 1. Improved Recall\n",
    "- BM25 catches exact keyword matches\n",
    "- Semantic search captures meaning even when wording differs  \n",
    "**Result:** fewer relevant documents are missed\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Handles Synonyms and Rephrasing\n",
    "- Semantic search matches:\n",
    "  - ‚Äúcreate app‚Äù ‚Üí ‚Äúbuild LLM system‚Äù\n",
    "- BM25 still ensures exact terms like `\"LLM\"` or `\"agent\"` are respected\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. More Robust to Query Styles\n",
    "Supports:\n",
    "- Short keyword queries: `\"LangChain agent\"`\n",
    "- Natural language questions:  \n",
    "  `\"How do I use LangChain to talk to tools?\"`\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Preserves Lexical Importance\n",
    "- BM25 emphasizes **rare and critical terms**\n",
    "- Essential in:\n",
    "  - Medical\n",
    "  - Legal\n",
    "  - Technical domains  \n",
    "Example: rare terms like `\"osteoporosis\"` should strongly influence ranking\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Adapts to Document Diversity\n",
    "Works well across:\n",
    "- Structured docs (APIs, specs)\n",
    "- Unstructured text (blogs, PDFs, notes)\n",
    "\n",
    "Hybrid retrieval adapts to varying writing styles and formats.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Easy to Tune\n",
    "You can control influence with weights:\n",
    "\n",
    "```text\n",
    "final_score = 0.7 * dense_score + 0.3 * sparse_score\n",
    "```\n",
    "\n",
    "## When to Use BM25 + Semantic Search Together?\n",
    "\n",
    "Hybrid retrieval (BM25 + semantic search) is most useful when queries, documents, or users vary in how precisely they express intent. Below are common use cases and why hybrid retrieval helps.\n",
    "\n",
    "---\n",
    "\n",
    "### Use Cases and Benefits\n",
    "\n",
    "#### RAG Pipelines\n",
    "**Why it helps:**  \n",
    "Prevents retrieval hallucinations by ensuring both **exact keyword matches** (BM25) and **fuzzy semantic matches** (dense) are considered.\n",
    "\n",
    "---\n",
    "\n",
    "#### Technical Documentation Search\n",
    "**Why it helps:**  \n",
    "Developers may search for *\"how to use API\"* while the documentation says *\"API usage\"*.  \n",
    "Using BM25 and semantic search together significantly improves hit rate.\n",
    "\n",
    "---\n",
    "\n",
    "#### Legal / Medical Question Answering\n",
    "**Why it helps:**  \n",
    "Some queries require **precise term matching** (BM25), while others rely on **general semantic understanding** (dense embeddings).  \n",
    "Hybrid retrieval supports both safely.\n",
    "\n",
    "---\n",
    "\n",
    "#### E-commerce / Product Search\n",
    "**Why it helps:**  \n",
    "Queries like *\"cheap noise-canceling headphones\"* can match  \n",
    "*\"affordable ANC earbuds\"* via semantic search, while BM25 confirms critical terms like `\"ANC\"`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Multilingual or Cross-Lingual Retrieval\n",
    "**Why it helps:**  \n",
    "Semantic models bridge language differences, while BM25 ensures exact matching when queries and documents share the same language.\n",
    "\n",
    "---\n",
    "\n",
    "#### Customer Support\n",
    "**Why it helps:**  \n",
    "Real users often type vague, keyword-heavy, or inconsistent queries.  \n",
    "Hybrid retrieval improves reliability in chatbots and FAQ systems.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d220cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_classic.retrievers import EnsembleRetriever\n",
    "from langchain_classic.schema import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb435226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sample Documents\n",
    "docs = [\n",
    "    Document(page_content=\"Langchain helps build LLM applications.\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for semantic search.\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris.\"),\n",
    "    Document(page_content=\"Langchain can be used to develop agentic ai application.\"),\n",
    "    Document(page_content=\"Langchain has many types of retrievers.\"),\n",
    "]\n",
    "\n",
    "# Step 2: Dense Retriever(FAISS + HuggingFace)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "dense_vector_store = FAISS.from_documents(docs, embedding_model)\n",
    "dense_retriever = dense_vector_store.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a121987",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3 : Sparce Retriever(BM25)\n",
    "sparse_retriever = BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k=3 ## top -k documents to retriever\n",
    "\n",
    "## Step 4: Combine with Ensemble Retriver\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[dense_retriever, sparse_retriever],\n",
    "    weights=[0.7, 0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8003670d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002136DB47140>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x0000021323DAC140>, k=3)], weights=[0.7, 0.3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dda792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " üîπ Document 1: \n",
      " Langchain helps build LLM applications.\n",
      "\n",
      " üîπ Document 2: \n",
      " Langchain can be used to develop agentic ai application.\n",
      "\n",
      " üîπ Document 3: \n",
      " Langchain has many types of retrievers.\n",
      "\n",
      " üîπ Document 4: \n",
      " Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Query and get results\n",
    "query = \"How can I build an application using LLM?\"\n",
    "results = hybrid_retriever.invoke(query)\n",
    "\n",
    "# Step 6: Print results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n üîπ Document {i+1}: \\n {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e37e4",
   "metadata": {},
   "source": [
    "### RAG Pipeline with hybrid retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b89afaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6657a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Prompt Template\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based on the context below.\n",
    "    Context:{context}\n",
    "    Question: {input}                                      \n",
    "\"\"\")\n",
    "\n",
    "# Step 6: LLM\n",
    "llm = init_chat_model(\n",
    "    model=\"groq:openai/gpt-oss-20b\",\n",
    "    temperature = 0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7598292e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002136DB47140>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x0000021323DAC140>, k=3)], weights=[0.7, 0.3]), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='Answer the question based on the context below.\\n    Context:{context}\\n    Question: {input}                                      \\n')\n",
       "            | ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000002136DBA1220>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000021370E4E450>, model_name='openai/gpt-oss-20b', temperature=0.4, model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Step 7: Create Stuff Document Chain\n",
    "document_chain = create_stuff_documents_chain(llm=llm , prompt=prompt)\n",
    "\n",
    "## Step 8: Create Full rag Chain\n",
    "rag_chain = create_retrieval_chain(retriever=hybrid_retriever,combine_docs_chain=document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeadf1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Answer: \n",
      " **Building an LLM‚Äëpowered app with LangChain (and optional Pinecone)**  \n",
      "Below is a practical, step‚Äëby‚Äëstep guide that shows how you can turn the concepts in the context into a working application.  \n",
      "Feel free to skip or expand any section depending on your skill level and the complexity you want.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Define the Problem & Scope\n",
      "\n",
      "| Question | Example |\n",
      "|----------|---------|\n",
      "| What is the app‚Äôs purpose? | ‚ÄúA chatbot that answers product‚Äërelated questions.‚Äù |\n",
      "| Who are the users? | Customer support agents. |\n",
      "| What data do we need? | Product FAQs, manuals, support tickets. |\n",
      "| What LLM do we want to use? | OpenAI GPT‚Äë4, Anthropic Claude, or a local Llama‚Äë2. |\n",
      "\n",
      "Having a clear problem statement lets you pick the right tools and architecture.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Set Up the Development Environment\n",
      "\n",
      "```bash\n",
      "# Create a virtual environment\n",
      "python -m venv llm_app\n",
      "source llm_app/bin/activate   # Windows: llm_app\\Scripts\\activate\n",
      "\n",
      "# Install core libraries\n",
      "pip install langchain==0.2.0  # adjust to latest stable\n",
      "pip install langchain-openai  # if you use OpenAI\n",
      "pip install langchain-anthropic  # if you use Anthropic\n",
      "pip install pinecone-client   # optional, for vector search\n",
      "pip install fastapi uvicorn   # optional, for a web API\n",
      "```\n",
      "\n",
      "> **Tip**: Use a `.env` file to store API keys securely.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Choose an LLM Provider\n",
      "\n",
      "```python\n",
      "from langchain_openai import ChatOpenAI\n",
      "# or\n",
      "# from langchain_anthropic import ChatAnthropic\n",
      "\n",
      "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.7)\n",
      "```\n",
      "\n",
      "*If you want a local model* (e.g., Llama‚Äë2), use `langchain-community`‚Äôs `ChatOllama`.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Prepare the Knowledge Base\n",
      "\n",
      "### 4.1 Load Documents\n",
      "\n",
      "```python\n",
      "from langchain.document_loaders import TextLoader\n",
      "loader = TextLoader(\"data/product_faq.txt\")\n",
      "docs = loader.load()\n",
      "```\n",
      "\n",
      "### 4.2 Split & Embed\n",
      "\n",
      "```python\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
      "texts = splitter.split_documents(docs)\n",
      "\n",
      "embeddings = OpenAIEmbeddings()\n",
      "```\n",
      "\n",
      "### 4.3 Store in a Retriever\n",
      "\n",
      "#### Option A: In‚Äëmemory (simple, no external DB)\n",
      "\n",
      "```python\n",
      "from langchain.vectorstores import FAISS\n",
      "faiss_index = FAISS.from_documents(texts, embeddings)\n",
      "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 4})\n",
      "```\n",
      "\n",
      "#### Option B: Pinecone (scalable, semantic search)\n",
      "\n",
      "```python\n",
      "import pinecone\n",
      "from langchain.vectorstores import Pinecone\n",
      "\n",
      "pinecone.init(api_key=\"YOUR_PINECONE_KEY\", environment=\"us-west1-gcp\")\n",
      "index_name = \"product-faq-index\"\n",
      "\n",
      "pinecone_index = pinecone.Index(index_name)\n",
      "pinecone_vectorstore = Pinecone.from_documents(\n",
      "    texts,\n",
      "    embeddings,\n",
      "    index_name=index_name,\n",
      "    namespace=\"faq\"\n",
      ")\n",
      "retriever = pinecone_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Build an Agent (Optional but Powerful)\n",
      "\n",
      "Agents let the LLM decide *what* to do (e.g., search, call an API, or generate a response).\n",
      "\n",
      "```python\n",
      "from langchain.agents import initialize_agent, AgentType\n",
      "from langchain.tools import Tool\n",
      "\n",
      "# Define a simple tool that uses the retriever\n",
      "def retrieve_tool(query: str) -> str:\n",
      "    docs = retriever.invoke(query)\n",
      "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
      "\n",
      "retrieve = Tool(\n",
      "    name=\"Retrieve\",\n",
      "    func=retrieve_tool,\n",
      "    description=\"Use this to fetch relevant documents for a query.\"\n",
      ")\n",
      "\n",
      "# Create the agent\n",
      "agent = initialize_agent(\n",
      "    tools=[retrieve],\n",
      "    llm=llm,\n",
      "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
      "    verbose=True\n",
      ")\n",
      "```\n",
      "\n",
      "Now the agent can decide when to call the `Retrieve` tool and when to generate a final answer.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Wrap It in a Web API (FastAPI)\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI, Request\n",
      "from pydantic import BaseModel\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "class Query(BaseModel):\n",
      "    question: str\n",
      "\n",
      "@app.post(\"/ask\")\n",
      "async def ask(query: Query):\n",
      "    # If using an agent\n",
      "    response = agent.run(query.question)\n",
      "    # If using a simple chain\n",
      "    # response = chain.run(question=query.question)\n",
      "    return {\"answer\": response}\n",
      "```\n",
      "\n",
      "Run with:\n",
      "\n",
      "```bash\n",
      "uvicorn main:app --reload\n",
      "```\n",
      "\n",
      "Now you can hit `http://localhost:8000/ask` with a JSON body `{\"question\":\"How do I reset my password?\"}`.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. (Optional) Add a Front‚ÄëEnd\n",
      "\n",
      "- **React** + **Vite** or **Next.js**  \n",
      "- **Streamlit** for a quick dashboard  \n",
      "- **Gradio** for a minimal UI\n",
      "\n",
      "```bash\n",
      "pip install streamlit\n",
      "```\n",
      "\n",
      "```python\n",
      "# streamlit_app.py\n",
      "import streamlit as st\n",
      "import requests\n",
      "\n",
      "st.title(\"Product FAQ Bot\")\n",
      "\n",
      "question = st.text_input(\"Ask a question\")\n",
      "if st.button(\"Send\"):\n",
      "    resp = requests.post(\"http://localhost:8000/ask\", json={\"question\": question})\n",
      "    st.write(resp.json()[\"answer\"])\n",
      "```\n",
      "\n",
      "Run with `streamlit run streamlit_app.py`.\n",
      "\n",
      "---\n",
      "\n",
      "## 8. Deploy\n",
      "\n",
      "| Platform | Steps |\n",
      "|----------|-------|\n",
      "| **Docker** | Write a `Dockerfile` that installs dependencies and runs `uvicorn`. |\n",
      "| **Heroku** | Use a `Procfile` with `web: uvicorn main:app --host 0.0.0.0 --port $PORT`. |\n",
      "| **AWS Lambda + API Gateway** | Use `Mangum` adapter. |\n",
      "| **Azure Functions** | Use `azure-functions` SDK. |\n",
      "\n",
      "---\n",
      "\n",
      "## 9. Monitoring & Scaling\n",
      "\n",
      "- **Logs**: `logging` module + CloudWatch / Loggly.  \n",
      "- **Metrics**: Track latency, token usage, error rates.  \n",
      "- **Scaling**: If you‚Äôre using Pinecone, you can increase the index size or switch to a higher‚Äëperformance plan.  \n",
      "- **Cost**: Keep an eye on token usage; consider caching frequent queries.\n",
      "\n",
      "---\n",
      "\n",
      "## 10. Quick Code Skeleton\n",
      "\n",
      "```python\n",
      "# main.py\n",
      "import os\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain.document_loaders import TextLoader\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.agents import initialize_agent, AgentType\n",
      "from langchain.tools import Tool\n",
      "from fastapi import FastAPI\n",
      "from pydantic import BaseModel\n",
      "\n",
      "# 1. LLM\n",
      "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.7)\n",
      "\n",
      "# 2. Data\n",
      "loader = TextLoader(\"data/product_faq.txt\")\n",
      "docs = loader.load()\n",
      "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
      "texts = splitter.split_documents(docs)\n",
      "embeddings = OpenAIEmbeddings()\n",
      "vectorstore = FAISS.from_documents(texts, embeddings)\n",
      "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
      "\n",
      "# 3. Tool & Agent\n",
      "def retrieve_tool(query: str) -> str:\n",
      "    docs = retriever.invoke(query)\n",
      "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
      "\n",
      "retrieve = Tool(\n",
      "    name=\"Retrieve\",\n",
      "    func=retrieve_tool,\n",
      "    description=\"Search the FAQ database.\"\n",
      ")\n",
      "\n",
      "agent = initialize_agent(\n",
      "    tools=[retrieve],\n",
      "    llm=llm,\n",
      "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
      "    verbose=True\n",
      ")\n",
      "\n",
      "# 4. API\n",
      "app = FastAPI()\n",
      "\n",
      "class Query(BaseModel):\n",
      "    question: str\n",
      "\n",
      "@app.post(\"/ask\")\n",
      "async def ask(query: Query):\n",
      "    answer = agent.run(query.question)\n",
      "    return {\"answer\": answer}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Recap\n",
      "\n",
      "1. **Define** the problem and data.  \n",
      "2. **Set up** LangChain + LLM + optional Pinecone.  \n",
      "3. **Load & embed** documents, store in a retriever.  \n",
      "4. **Build** an agent (or simple chain).  \n",
      "5. **Expose** via FastAPI (or any web framework).  \n",
      "6. **Deploy** and monitor.\n",
      "\n",
      "With LangChain‚Äôs modular design, you can swap out any component (LLM, embeddings, vector store, tools) without rewriting the whole app. Happy building!\n",
      "\n",
      "üìÉ Source Documents: \n",
      "\n",
      " Doc 1: Langchain helps build LLM applications.\n",
      "\n",
      " Doc 2: Langchain can be used to develop agentic ai application.\n",
      "\n",
      " Doc 3: Langchain has many types of retrievers.\n",
      "\n",
      " Doc 4: Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Ask a question\n",
    "query = {\"input\": \"How can I build an app using LLMs?\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "# Step 10 : Output\n",
    "print(\"‚úîÔ∏è Answer: \\n\", response[\"answer\"])\n",
    "\n",
    "print(\"\\nüìÉ Source Documents: \")\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(f\"\\n Doc {i+1}: {doc.page_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_UDEMY (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
