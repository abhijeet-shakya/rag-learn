{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d3fa47",
   "metadata": {},
   "source": [
    "### Semantic Chunking \n",
    "- Semantic Chunker is a document splitter that uses embedding similarity between sentences to decide chunk boundries\n",
    "- It ensures that each chunk is semantically coherent and not cut off mid-thought like traditional character/token splitters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67889e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5642fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Langchain is a framework for building application with LLMs.', 'Langchain Provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.', 'You can create chains, agents, memory, and retrievers.', 'The Eiffel Tower is loacated in Paris.', 'France is a popular tourist destination.']\n",
      "Chunks: []\n",
      "current_chunk: ['Langchain is a framework for building application with LLMs.', 'Langchain Provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.']\n",
      "Chunks: ['Langchain is a framework for building application with LLMs. Langchain Provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.']\n",
      "current_chunk: ['You can create chains, agents, memory, and retrievers.']\n",
      "Chunks: ['Langchain is a framework for building application with LLMs. Langchain Provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.', 'You can create chains, agents, memory, and retrievers.']\n",
      "current_chunk: ['The Eiffel Tower is loacated in Paris.']\n",
      "Chunks: ['Langchain is a framework for building application with LLMs. Langchain Provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.', 'You can create chains, agents, memory, and retrievers.', 'The Eiffel Tower is loacated in Paris.']\n",
      "current_chunk: ['France is a popular tourist destination.']\n",
      "['Langchain is a framework for building application with LLMs. Langchain Provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.', 'You can create chains, agents, memory, and retrievers.', 'The Eiffel Tower is loacated in Paris.']\n",
      "['Langchain is a framework for building application with LLMs. Langchain Provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.', 'You can create chains, agents, memory, and retrievers.', 'The Eiffel Tower is loacated in Paris.', 'France is a popular tourist destination.']\n",
      "\n",
      "ðŸ“Œ Semantic Chunks:\n",
      "\n",
      "Chunk 1: \n",
      "Langchain is a framework for building application with LLMs. Langchain Provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
      "\n",
      "Chunk 2: \n",
      "You can create chains, agents, memory, and retrievers.\n",
      "\n",
      "Chunk 3: \n",
      "The Eiffel Tower is loacated in Paris.\n",
      "\n",
      "Chunk 4: \n",
      "France is a popular tourist destination.\n"
     ]
    }
   ],
   "source": [
    "## Intialize the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "## Sample text\n",
    "text = \"\"\"\n",
    "Langchain is a framework for building application with LLMs.\n",
    "Langchain Provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
    "You can create chains, agents, memory, and retrievers.\n",
    "The Eiffel Tower is loacated in Paris.\n",
    "France is a popular tourist destination.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "## Step 1:Split into sentences\n",
    "\n",
    "sentences = [s.strip() for s in text.split(\"\\n\") if s.strip()]\n",
    "print(sentences)\n",
    "## Step 2: Embed each sentence\n",
    "embedding = model.encode(sentences)\n",
    "# print(embedding)\n",
    "# Step 3: Initialize parameters\n",
    "threshold = 0.7\n",
    "chunks = []\n",
    "current_chunk=[sentences[0]]\n",
    "\n",
    "## Step 4: Semantic grouping basend on threshold\n",
    "\n",
    "for i in range(1,len(sentences)):\n",
    "    sim = cosine_similarity(\n",
    "        [embedding[i-1]],\n",
    "        [embedding[i]]\n",
    "    )[0][0]\n",
    "\n",
    "    if sim >= threshold:\n",
    "        current_chunk.append(sentences[i])\n",
    "        print(f\"Chunks: {chunks}\")\n",
    "        print(f\"current_chunk: {current_chunk}\")\n",
    "    else:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        current_chunk=[sentences[i]]\n",
    "        print(f\"Chunks: {chunks}\")\n",
    "        print(f\"current_chunk: {current_chunk}\")\n",
    "\n",
    "print(chunks)\n",
    "# Append the last chunk\n",
    "chunks.append(\" \".join(current_chunk))\n",
    "print(chunks)\n",
    "# output chunks\n",
    "print(\"\\nðŸ“Œ Semantic Chunks:\")\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {idx+1}: \\n{chunk}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212abdfc",
   "metadata": {},
   "source": [
    "### RAG Pipeline in Modular Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16062cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_classic.schema import Document\n",
    "from langchain_classic.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_classic.schema.runnable import RunnableLambda, RunnableMap\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7e9e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom semantic chunker with Threshold\n",
    "\n",
    "class ThresholdSemanticChunker:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", threshold=0.7):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def split(self, text: str):\n",
    "        sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "        embeddings = self.model.encode(sentences)\n",
    "        chunks = []\n",
    "        current_chunk = [sentences[0]]\n",
    "\n",
    "        for i in range(1,len(sentences)):\n",
    "            sim = cosine_similarity([embeddings[i-1]], [embeddings[i]])[0][0]\n",
    "            if sim >= self.threshold:\n",
    "                current_chunk.append(sentences[i])\n",
    "            else:\n",
    "                chunks.append(\". \".join(current_chunk) + \".\")\n",
    "                current_chunk = [sentences[i]]\n",
    "        chunks.append(\". \".join(current_chunk) + \".\")\n",
    "        return chunks\n",
    "    \n",
    "    def split_doucments(self, docs):\n",
    "        result=[]\n",
    "        for doc in docs:\n",
    "            for chunk in  self.split(doc.page_content):\n",
    "                result.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "                \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f5c2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='\\nLangchain is a framework for building application with LLMs.\\nLangchain Provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\\nYou can create chains, agents, memory, and retrievers.\\nThe Eiffel Tower is loacated in Paris.\\nFrance is a popular tourist destination.\\n')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sample text\n",
    "sample_text = \"\"\"\n",
    "Langchain is a framework for building application with LLMs.\n",
    "Langchain Provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
    "You can create chains, agents, memory, and retrievers.\n",
    "The Eiffel Tower is loacated in Paris.\n",
    "France is a popular tourist destination.\n",
    "\"\"\"\n",
    "\n",
    "doc = Document(page_content=sample_text)\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e12bd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Langchain is a framework for building application with LLMs. Langchain Provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.'),\n",
       " Document(metadata={}, page_content='You can create chains, agents, memory, and retrievers.'),\n",
       " Document(metadata={}, page_content='The Eiffel Tower is loacated in Paris.'),\n",
       " Document(metadata={}, page_content='France is a popular tourist destination.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chunkking \n",
    "chunker = ThresholdSemanticChunker(threshold=0.7)\n",
    "chunks = chunker.split_doucments([doc])\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "610283e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vector Store\n",
    "from  langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "## Inialize a simple Embedding model(no API key needed!)\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "vector_store = FAISS.from_documents(chunks,embedding)\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "832bc3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the following context:\\n{context}\\nQuestion: {question}\\n')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prompt Template \n",
    "template = \"\"\"\n",
    "Answer the question based on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdbb91fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework for building applications that use large language models (LLMs). It provides modular abstractions to combine LLMs with tools such as OpenAI and Pinecone, enabling developers to create chains, agents, memory, and retrievers for more powerful, integrated AI solutions.\n"
     ]
    }
   ],
   "source": [
    "### LLM \n",
    "llm = init_chat_model(\n",
    "    model=\"groq:openai/gpt-oss-20b\",\n",
    "    temperature = 0.4\n",
    ")\n",
    "\n",
    "### LCEL chain with retrival\n",
    "rag_chain=(\n",
    "    RunnableMap(\n",
    "        {\n",
    "            \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "            \"question\": lambda x: x[\"question\"]\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query={\"question\": \"what is Langchain used for?\"}\n",
    "result = rag_chain.invoke(query)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9896b07b",
   "metadata": {},
   "source": [
    "### Semantic Chunker with Langchain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d40b8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_classic.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26bb1713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " chunk 1 \n",
      " Langchain is a framework for building application with LLMs. Langchain Provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone. You can create chains, agents, memory, and retrievers.\n",
      "\n",
      " chunk 2 \n",
      " The Eiffel Tower is loacated in Paris. France is a popular tourist destination. \n"
     ]
    }
   ],
   "source": [
    "### Load the documents\n",
    "loader= TextLoader(\"langchain_intro.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "## Intialize embedding model\n",
    "## Inialize a simple Embedding model(no API key needed!)\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "## Create the semantic chunker\n",
    "chunker = SemanticChunker(embedding)\n",
    "\n",
    "## Split the documents\n",
    "chunks=chunker.split_documents(docs)\n",
    "\n",
    "## Result \n",
    "for i,chunk in enumerate(chunks):\n",
    "    print(f\"\\n chunk {i+1} \\n {chunk.page_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_UDEMY (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
