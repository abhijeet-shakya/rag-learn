{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b907e379",
   "metadata": {},
   "source": [
    "## üß† What is Query Decompostion?\n",
    "Query decomposition is the process of taking a complex, multipart question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "### ‚úÖ Why use Query Decompostion?\n",
    "- Complex queries oftern involve multiple concepts\n",
    "- LLMs or retrievers may miss parts of the original Question\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "- Allow parallelism (especially in multi-agent frameworks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b41fbe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.document_loaders import TextLoader\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddca229",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Setup Retriver\n",
    "# TextLoader\n",
    "loader = TextLoader(\"langchain.txt\", encoding=\"utf-8\", autodetect_encoding=True)\n",
    "raw_docs = loader.load()\n",
    "\n",
    "# Split the documents\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "\n",
    "# Embedding model and vectore store\n",
    "embedding_model = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# make MMR retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type = \"mmr\",\n",
    "    search_kwargs = {\"k\":4, \"lambda_mult\": 0.7}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d5ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001C055C0EF60>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C055C2BE00>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 2: LLM\n",
    "# LLM and Prompt\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = init_chat_model(\"groq:openai/gpt-oss-120b\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dd3fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3:  Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are and AI assitant. Decompose the following complex question into 2 to 4 smaller sub-question for better document retrival.\n",
    "Question : {question}\n",
    "Sub-questions:\n",
    "\"\"\"\n",
    ")\n",
    "decompostion_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233000cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does Langchain use memory and agents compared to CrewAI?\"\n",
    "decompostion_question = decompostion_chain.invoke({\"question\":query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e7f1692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sub‚Äëquestions**\n",
      "\n",
      "1. What memory mechanisms and types does LangChain offer, and how are they used within LangChain applications?  \n",
      "2. How does LangChain implement and manage agents (e.g., tool‚Äëusing agents, planning agents, routing agents)?  \n",
      "3. What memory capabilities and agent architectures are provided by CrewAI, and how are they applied in CrewAI workflows?  \n",
      "4. In what ways do the memory handling and agent designs of LangChain differ from those of CrewAI (e.g., architecture, extensibility, integration with LLMs, runtime behavior)?\n"
     ]
    }
   ],
   "source": [
    "print(decompostion_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1596f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Use the context below to answer the question.\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {input}\n",
    "    \"\"\"\n",
    ")\n",
    "qa_chain = create_stuff_documents_chain(llm=llm , prompt=qa_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "256b0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline login\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decompostion_chain.invoke({\"question\":user_query})\n",
    "    sub_questions = [q.strip(\"-.1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip().startswith((\"1.\",\"2.\",\"3.\",\"4.\"))]\n",
    "    \n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = qa_chain.invoke({\"input\":subq, \"context\":docs})\n",
    "        results.append(f\"Q: {subq} \\n : {result}\")\n",
    "    return \"\\n\\n\".join(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fe711eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final Answer: \n",
      "\n",
      "Q: **What types of memory mechanisms does LangChain provide, and how are they implemented in its workflows?** \n",
      " : **LangChain‚Äôs memory layer** is the part of the library that lets a chain or an agent keep track of what has happened earlier in the conversation (or in a multi‚Äëstep workflow) and feed that context back into the LLM on subsequent calls.  \n",
      "LangChain ships with a handful of ready‚Äëmade memory classes, each of which implements the same `BaseMemory` interface (methods `load_memory_variables`, `save_context`, and `clear`).  By swapping one of these objects into a chain/agent you change how the historic information is stored, summarized, or retrieved without touching the rest of the workflow.\n",
      "\n",
      "Below is a concise catalogue of the **main memory mechanisms** that LangChain provides today (as of the 2024 release), together with a short description of **how they are wired into a workflow**.\n",
      "\n",
      "| Memory class (type) | What it does / when to use it | Key implementation details | Typical workflow integration |\n",
      "|---------------------|------------------------------|----------------------------|------------------------------|\n",
      "| **ConversationBufferMemory** | Stores the raw sequence of user‚Äëassistant turns in a simple list. Ideal for short chats where you want the full transcript. | ‚Ä¢ Keeps an in‚Äëmemory list `messages = []`.<br>‚Ä¢ `load_memory_variables` returns a single string `chat_history` (joined with a separator).<br>‚Ä¢ Optionally limits length with `k` (keep last *k* turns). | Pass the instance to `LLMChain` or `ChatOpenAI` via the `memory=` argument. The chain automatically appends each new `(input, output)` pair and injects `chat_history` into the prompt template. |\n",
      "| **ConversationSummaryMemory** | Summarizes the conversation after each turn so the LLM sees a concise recap instead of the full transcript. Good for long‚Äërunning dialogs where token budget is tight. | ‚Ä¢ After each turn, calls a *summary LLM* (often the same model) with a prompt like ‚ÄúSummarize the conversation so far‚Äù.<br>‚Ä¢ Stores the resulting summary string. <br>‚Ä¢ `load_memory_variables` returns the latest summary. | Used the same way as the buffer memory, but you also supply a `summarize_prompt` (or rely on the default). The chain will call `self.llm` internally to keep the summary up‚Äëto‚Äëdate. |\n",
      "| **ConversationSummaryBufferMemory** | Hybrid of the two above: keeps the last *k* raw turns **plus** a running summary of everything before them. Balances fidelity with brevity. | ‚Ä¢ Maintains both a buffer (`messages[-k:]`) and a `summary` string.<br>‚Ä¢ When the buffer exceeds *k*, the oldest turn is folded into the summary via the summarizer LLM. | Same injection pattern; the prompt template can reference both `summary` and `history`. Useful for agents that need the most recent context verbatim while also remembering the overall goal. |\n",
      "| **ConversationEntityMemory** | Tracks named entities (people, places, objects, etc.) that appear in the dialogue and stores a short description for each. Helpful for ‚Äúcharacter‚Äëdriven‚Äù agents or task‚Äëoriented bots. | ‚Ä¢ Uses an entity‚Äëextraction LLM (or a regex/NER tool) on each user turn.<br>‚Ä¢ Stores a dict `{entity_name: description}` in memory.<br>‚Ä¢ `load_memory_variables` returns the dict (or a formatted string). | You typically add it to a `ConversationalRetrievalChain` where the prompt contains a placeholder like `{entity_memory}`. The chain updates the dict after every turn and the LLM can refer back to known entities. |\n",
      "| **VectorStoreRetrieverMemory** | Persists the entire conversation (or any intermediate artifacts) in a vector store (FAISS, Chroma, Pinecone, etc.) and retrieves the most relevant past snippets on demand. Works when you need semantic recall rather than chronological. | ‚Ä¢ After each turn, the text is embedded with the same embedding model used for the external knowledge base.<br>‚Ä¢ Embeddings are added to the vector store.<br>‚Ä¢ `load_memory_variables` runs a similarity search (`retriever.get_relevant_documents`) using the current user query. | Often combined with a **retrieval‚Äëaugmented generation** chain: the memory acts as a *second* retriever that pulls from the conversation history, while the main retriever pulls from the document corpus. You pass the memory object to the chain‚Äôs `memory=` argument; the chain automatically calls `memory.load_memory_variables` before building the final prompt. |\n",
      "| **CombinedMemory** | Allows you to compose several memory objects (e.g., buffer + vector store) into one logical memory unit. | ‚Ä¢ Holds a list of `BaseMemory` instances.<br>‚Ä¢ `load_memory_variables` merges the dictionaries returned by each sub‚Äëmemory (keys must be distinct).<br>‚Ä¢ `save_context` forwards the `(input, output)` pair to every sub‚Äëmemory. | Very handy when you want both a raw transcript (`chat_history`) and a semantic recall (`retrieved_memories`). You give the combined memory to the chain; the prompt can reference both keys. |\n",
      "| **ConversationBufferWindowMemory** (a variant of Buffer) | Keeps a sliding window of the *most recent* *N* tokens rather than a fixed number of turns. Useful when you need strict token‚Äëbudget control. | ‚Ä¢ Internally tracks token count (via the model‚Äôs tokenizer).<br>‚Ä¢ Drops oldest turns until the token budget is satisfied. | Plug‚Äëin exactly like `ConversationBufferMemory`; the only difference is the window size is expressed in tokens (`max_token_limit`). |\n",
      "| **File/SQLite/Redis Memory** (persistent back‚Äëends) | Persists the memory to disk or a key‚Äëvalue store so the state survives process restarts. | ‚Ä¢ Implements `BaseMemory` but uses a storage layer (JSON file, SQLite DB, Redis hash).<br>‚Ä¢ On `load_memory_variables` it reads the persisted state; on `save_context` it writes back. | You instantiate the persistent class (e.g., `ConversationBufferMemory(memory_key=\"chat_history\", chat_memory=SQLiteChatMessageHistory(...))`) and pass it to the chain. The rest of the workflow stays unchanged. |\n",
      "| **AgentMemory** (used by `AgentExecutor`) | Stores tool‚Äëuse history, intermediate reasoning steps, and final actions for a multi‚Äëstep agent. | ‚Ä¢ Keeps a list of `AgentStep` objects (action, observation).<br>‚Ä¢ Provides `format_agent_steps` to inject a ‚Äúthought‚Äëprocess‚Äù block into the next prompt. | When you create an `AgentExecutor` you can supply `memory=AgentMemory()`; the executor automatically calls `memory.save_context` after each tool call and adds the formatted steps to the next LLM prompt. |\n",
      "\n",
      "---\n",
      "\n",
      "### How Memory is **wired into LangChain workflows**\n",
      "\n",
      "1. **Create the memory object**  \n",
      "   ```python\n",
      "   from langchain.memory import ConversationBufferMemory\n",
      "   memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
      "   ```\n",
      "\n",
      "2. **Pass it to a chain / agent**  \n",
      "   ```python\n",
      "   from langchain.chains import LLMChain\n",
      "   from langchain.prompts import PromptTemplate\n",
      "   prompt = PromptTemplate(\n",
      "       input_variables=[\"input\", \"chat_history\"],\n",
      "       template=\"{chat_history}\\nUser: {input}\\nAssistant:\"\n",
      "   )\n",
      "   chain = LLMChain(llm=ChatOpenAI(), prompt=prompt, memory=memory)\n",
      "   ```\n",
      "\n",
      "   *When `chain.run(user_input)` is called:*\n",
      "   - The chain calls `memory.load_memory_variables({\"input\": user_input})`.  \n",
      "   - The returned dict (e.g., `{\"chat_history\": \"...previous turns...\"}`) is merged into the prompt variables.\n",
      "   - The LLM generates a response.\n",
      "   - The chain then calls `memory.save_context({\"input\": user_input}, {\"output\": response})` so the new turn is stored.\n",
      "\n",
      "3. **In a Retrieval‚ÄëAugmented Generation (RAG) pipeline**  \n",
      "   ```python\n",
      "   from langchain.chains import ConversationalRetrievalChain\n",
      "   rag = ConversationalRetrievalChain.from_llm(\n",
      "       llm=ChatOpenAI(),\n",
      "       retriever=document_retriever,\n",
      "       memory=VectorStoreRetrieverMemory(vectorstore=conversation_store)\n",
      "   )\n",
      "   ```\n",
      "\n",
      "   - `load_memory_variables` performs a similarity search against the *conversation* vector store, returning e.g. `{\"retrieved_memories\": [doc1, doc2]}`.\n",
      "   - The prompt template can now contain both `{retrieved_memories}` (external docs) and `{chat_history}` (or `{retrieved_memories}` from memory) to give the LLM a richer context.\n",
      "\n",
      "4. **Agent executor with memory**  \n",
      "   ```python\n",
      "   from langchain.agents import initialize_agent, AgentType\n",
      "   agent = initialize_agent(\n",
      "       tools=my_tools,\n",
      "       llm=ChatOpenAI(),\n",
      "       agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
      "       memory=AgentMemory()\n",
      "   )\n",
      "   ```\n",
      "\n",
      "   - After each tool call, `AgentMemory.save_context` records the action/observation.\n",
      "   - The next LLM prompt automatically receives a formatted ‚ÄúThought‚ÄëProcess‚Äù block that includes the full reasoning chain, enabling the agent to stay coherent over many steps.\n",
      "\n",
      "5. **Persisting across sessions**  \n",
      "   ```python\n",
      "   from langchain.memory import ConversationBufferMemory\n",
      "   from langchain.schema import SQLiteChatMessageHistory\n",
      "   history = SQLiteChatMessageHistory(db_path=\"chat.db\")\n",
      "   memory = ConversationBufferMemory(chat_memory=history)\n",
      "   ```\n",
      "\n",
      "   - The `SQLiteChatMessageHistory` object implements the same `BaseChatMessageHistory` interface used by memory; it reads/writes rows in a SQLite table.\n",
      "   - When the application restarts, `memory.load_memory_variables` pulls the prior transcript from the DB, so the conversation can continue seamlessly.\n",
      "\n",
      "---\n",
      "\n",
      "### Summary\n",
      "\n",
      "- **Types**: buffer, summary, buffer‚Äësummary hybrid, entity‚Äëtracking, vector‚Äëstore semantic recall, combined/composite, window‚Äëtoken buffer, persistent (file/DB/Redis), and agent‚Äëspecific step memory.\n",
      "- **Implementation pattern**: each memory class adheres to `BaseMemory`. In a chain/agent you supply the memory via the `memory=` argument; the framework automatically calls `load_memory_variables` before the LLM runs and `save_context` after it runs. This makes memory a plug‚Äëand‚Äëplay component that can be swapped, stacked, or persisted without rewriting the core prompt logic.\n",
      "- **Result**: By selecting the appropriate memory mechanism, developers can give LangChain‚Äëbased RAG or agentic systems the ability to remember raw dialogue, maintain concise summaries, retrieve past semantics, track entities, or keep tool‚Äëuse histories‚Äîleading to more coherent, context‚Äëaware, and token‚Äëefficient applications.\n",
      "\n",
      "Q: **How does LangChain‚Äôs agent framework operate (e.g., tool‚Äëcalling, planning, and execution), and what are its key features?** \n",
      " : **LangChain‚ÄØAgent Framework ‚Äì Overview**\n",
      "\n",
      "LangChain‚Äôs *agent* abstraction is the glue that lets a language model (LLM) act like an autonomous ‚Äúassistant‚Äù that can **plan**, **call tools**, and **execute** a multi‚Äëstep workflow to answer a user request.  \n",
      "At a high level the framework works as a **loop**:\n",
      "\n",
      "1. **Input ‚Üí Prompt** ‚Äì The user query (or the output of a previous step) is inserted into a *prompt template* that tells the LLM it is an ‚Äúagent‚Äù and lists the tools it may use.  \n",
      "2. **LLM‚ÄØ‚Üí‚ÄØPlan** ‚Äì The LLM generates a **structured response** (usually JSON or a simple DSL) that specifies:\n",
      "   * What tool to call (or ‚Äúnone‚Äù if the answer can be produced directly).  \n",
      "   * The arguments for that tool.  \n",
      "   * Optionally, a ‚Äúthought‚Äù or ‚Äúreasoning‚Äù string that can be logged for transparency.  \n",
      "3. **Tool‚ÄëCalling** ‚Äì The framework parses the LLM‚Äôs response, looks up the requested tool in a registry, and invokes it with the supplied arguments.  \n",
      "4. **Result ‚Üí Observation** ‚Äì The tool returns its output (e.g., a web‚Äësearch result, a database row, a calculation). The framework packages this as an **observation** and feeds it back to the LLM.  \n",
      "5. **Iteration** ‚Äì Steps‚ÄØ2‚Äë4 repeat until the LLM decides it has enough information and emits a final answer (or hits a max‚Äëstep limit).  \n",
      "\n",
      "The loop is sometimes called the **ReAct** (Reason‚ÄØ‚Üí‚ÄØAct) pattern, because the model alternates between reasoning (‚ÄúI need to look up ‚Ä¶‚Äù) and acting (calling a tool).\n",
      "\n",
      "---\n",
      "\n",
      "### Core Concepts & Building Blocks\n",
      "\n",
      "| Component | What it is | Typical Use |\n",
      "|-----------|------------|-------------|\n",
      "| **Agent** | An object that orchestrates the LLM‚Äëtool loop. | `AgentExecutor`, `ChatAgent`, `ZeroShotAgent`, etc. |\n",
      "| **Tool** | A thin wrapper around any callable (function, API client, chain, etc.) that the LLM can invoke. | Search, calculator, SQL query, custom Python function, retrieval chain, etc. |\n",
      "| **PromptTemplate / ChatPromptTemplate** | Structured prompt that injects the list of tools, the user input, and the previous reasoning steps. | Provides the ‚Äúagent‚Äëaware‚Äù context the model needs. |\n",
      "| **LLM** | Any language‚Äëmodel provider supported by LangChain (OpenAI, Anthropic, Llama‚Äë2, etc.). | Generates the planning JSON. |\n",
      "| **Memory** | Optional state that persists across turns (e.g., conversation history, retrieved docs). | `ConversationBufferMemory`, `VectorStoreRetrieverMemory`. |\n",
      "| **CallbackManager** | Hook system for logging, tracing, or visualising each step (LLM call, tool call, etc.). | Debugging, observability, UI dashboards. |\n",
      "| **OutputParser** | Parses the LLM‚Äôs structured response into a concrete `AgentAction` (tool name + args) or `AgentFinish`. | Usually `ReActOutputParser` or a custom parser. |\n",
      "| **Planner (optional)** | A higher‚Äëlevel component that can break a complex request into sub‚Äëtasks before the agent loop starts. | Used in CrewAI‚Äëstyle multi‚Äëagent pipelines. |\n",
      "\n",
      "---\n",
      "\n",
      "### How Tool‚ÄëCalling Works\n",
      "\n",
      "1. **Tool Registration** ‚Äì When you instantiate an agent you pass a list of `Tool` objects. Each `Tool` defines:\n",
      "   * `name` (string used in the LLM‚Äôs plan)  \n",
      "   * `func` (callable that receives a string argument and returns a string)  \n",
      "   * `description` (human‚Äëreadable text that appears in the prompt)  \n",
      "\n",
      "2. **Prompt Generation** ‚Äì LangChain builds a prompt that looks roughly like:\n",
      "\n",
      "   ```\n",
      "   You are an AI assistant with access to the following tools:\n",
      "   - search: Useful for looking up information on the web.\n",
      "   - calculator: Useful for arithmetic.\n",
      "   ...\n",
      "   Question: {user_input}\n",
      "   Thought: ...\n",
      "   Action: <tool name>\n",
      "   Action Input: <args>\n",
      "   Observation: ...\n",
      "   ```\n",
      "\n",
      "3. **LLM Output** ‚Äì The model replies with a block such as:\n",
      "\n",
      "   ```json\n",
      "   {\"action\": \"search\", \"action_input\": \"latest research on quantum error correction\"}\n",
      "   ```\n",
      "\n",
      "   or in ReAct style:\n",
      "\n",
      "   ```\n",
      "   Thought: I should look up recent papers.\n",
      "   Action: search\n",
      "   Action Input: latest research on quantum error correction\n",
      "   ```\n",
      "\n",
      "4. **Execution** ‚Äì The framework extracts `action` and `action_input`, looks up the `search` tool, runs `search(action_input)`, captures the result, and appends it as an `Observation` for the next LLM turn.\n",
      "\n",
      "5. **Termination** ‚Äì When the LLM outputs a `final answer` (often signaled by a special key like `\"final_answer\"` or the word ‚ÄúAnswer‚Äù), the loop ends and the answer is returned to the caller.\n",
      "\n",
      "---\n",
      "\n",
      "### Planning & Multi‚ÄëStep Reasoning\n",
      "\n",
      "* **Zero‚ÄëShot React Agent** ‚Äì The simplest agent that relies on the LLM‚Äôs internal reasoning to decide which tool to call next. No external planner is needed.\n",
      "* **Structured / Function‚ÄëCalling Agents** ‚Äì Use the newer ‚Äúfunction calling‚Äù API (OpenAI, Anthropic) where the LLM directly returns a JSON payload matching a predefined schema. LangChain maps that schema to tool calls.\n",
      "* **Planner‚ÄëBased Agents** ‚Äì For very complex tasks, LangChain can first invoke a *planner* LLM that decomposes the request into a list of sub‚Äëqueries. Each sub‚Äëquery is then fed to a regular agent (or a different specialized agent) and the results are aggregated. This pattern mirrors the *CrewAI* workflow described in the context.\n",
      "\n",
      "---\n",
      "\n",
      "### Key Features of LangChain‚Äôs Agent Framework\n",
      "\n",
      "| Feature | What it Gives You |\n",
      "|---------|-------------------|\n",
      "| **Unified Tool Interface** | Any Python callable, external API, or LangChain chain can be exposed as a tool with just a name & description. |\n",
      "| **Dynamic Tool Selection** | The LLM decides *at runtime* which tool to use, enabling flexible, data‚Äëdriven behavior. |\n",
      "| **ReAct Loop (Reason‚ÄØ‚Üí‚ÄØAct)** | Built‚Äëin support for the iterative reasoning‚Äëaction pattern that yields strong multi‚Äëstep performance. |\n",
      "| **Function‚ÄëCalling Integration** | Native support for OpenAI/Anthropic function‚Äëcalling, allowing strict JSON output and type safety. |\n",
      "| **Memory & State** | Optional memory modules let agents remember prior observations, conversation history, or retrieved documents across turns. |\n",
      "| **Prompt Templates & Custom Parsers** | Easy to swap in your own prompt style (e.g., few‚Äëshot examples, system messages) and parse custom output formats. |\n",
      "| **Callback & Tracing System** | Hooks for logging LLM calls, tool invocations, token usage, and visualising the entire reasoning chain. |\n",
      "| **Composable Agents** | Agents can be nested or chained (e.g., a ‚Äúdecomposer‚Äù agent feeds sub‚Äëqueries to a ‚Äúretriever‚Äù agent), supporting multi‚Äëagent coordination. |\n",
      "| **Extensible Planner Layer** | You can plug a higher‚Äëlevel planner (CrewAI‚Äëstyle) that creates a task graph before the agent loop runs. |\n",
      "| **Safety & Guardrails** | Built‚Äëin validation of tool arguments, sandboxed execution, and the ability to filter or block certain actions. |\n",
      "| **Multi‚ÄëModal & Retrieval‚ÄëAugmented** | Agents can be paired with retrievers (vector stores) so that a tool can be ‚Äúretrieve‚Äëdocuments‚Äù and the LLM can reason over those docs. |\n",
      "| **Deployment‚ÄëReady** | Agents can be run locally, on serverless functions, or via LangServe/LangChain‚ÄëServe for HTTP APIs. |\n",
      "\n",
      "---\n",
      "\n",
      "### Typical Code Sketch (Python)\n",
      "\n",
      "```python\n",
      "from langchain.agents import initialize_agent, Tool\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.utilities import SerpAPIWrapper, Calculator\n",
      "\n",
      "# Define tools\n",
      "search = Tool(\n",
      "    name=\"search\",\n",
      "    func=SerpAPIWrapper().run,\n",
      "    description=\"Search the web for up‚Äëto‚Äëdate information.\"\n",
      ")\n",
      "\n",
      "calc = Tool(\n",
      "    name=\"calculator\",\n",
      "    func=Calculator().run,\n",
      "    description=\"Perform arithmetic calculations.\"\n",
      ")\n",
      "\n",
      "tools = [search, calc]\n",
      "\n",
      "# LLM (can be any LangChain‚Äëcompatible model)\n",
      "llm = OpenAI(temperature=0)\n",
      "\n",
      "# Build an agent that uses the ReAct loop\n",
      "agent = initialize_agent(\n",
      "    tools,\n",
      "    llm,\n",
      "    agent=\"zero-shot-react-description\",  # or \"openai-functions\", \"structured-chat-zero-shot-react\"\n",
      "    verbose=True\n",
      ")\n",
      "\n",
      "# Run a query\n",
      "answer = agent.run(\n",
      "    \"What is the current price of Bitcoin, and how many times larger is the market cap of Apple compared to it?\"\n",
      ")\n",
      "\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "*The `initialize_agent` helper wires together the prompt template, output parser, tool registry, and the ReAct execution loop.*\n",
      "\n",
      "---\n",
      "\n",
      "### Putting It All Together\n",
      "\n",
      "1. **Planning** ‚Äì The LLM, guided by a prompt that lists available tools, decides *what* to do next.  \n",
      "2. **Tool‚ÄëCalling** ‚Äì The framework turns the LLM‚Äôs plan into a concrete function call, runs the tool, and captures the observation.  \n",
      "3. **Execution Loop** ‚Äì The LLM receives the observation, updates its internal reasoning, and either repeats the cycle or produces the final answer.  \n",
      "4. **Features** ‚Äì Modularity, memory, callbacks, function‚Äëcalling, multi‚Äëagent composition, and safety controls make LangChain agents a versatile backbone for retrieval‚Äëaugmented generation, autonomous assistants, and any workflow where an LLM must interact with external resources.\n",
      "\n",
      "In short, LangChain‚Äôs agent framework provides a **plug‚Äëand‚Äëplay architecture** that lets a language model **plan**, **act**, and **iterate** over tools and data sources, while giving developers fine‚Äëgrained control over prompts, memory, observability, and multi‚Äëagent orchestration. This is why it has become the de‚Äëfacto standard for building sophisticated, tool‚Äëenabled LLM applications.\n",
      "\n",
      "Q: **What memory capabilities does CrewAI offer, and how are they structured within its architecture?** \n",
      " : **CrewAI‚Äôs memory system is built as a hierarchy of three complementary stores that let individual agents, the whole crew, and the long‚Äëterm knowledge base keep track of what has happened and what still needs to be done.**\n",
      "\n",
      "| Level | What it stores | Who can read / write | How it is wired into the architecture |\n",
      "|-------|----------------|----------------------|---------------------------------------|\n",
      "| **1. Agent‚Äëlevel (short‚Äëterm) memory** | The immediate context of a single agent ‚Äì the last few messages, tool results, intermediate variables, and the agent‚Äôs own ‚Äúthought chain‚Äù. | Only the owning agent (and the orchestrator when it explicitly forwards the state). | Implemented as a lightweight `AgentMemory` object attached to each `CrewAgent`. It is automatically passed to the LLM on every turn and cleared when the agent finishes its sub‚Äëtask. |\n",
      "| **2. Crew‚Äëlevel (shared) memory** | A transient workspace that all agents can contribute to ‚Äì task progress flags, shared hypotheses, partial answers, and coordination signals. | Any agent in the crew (read/write) and the Crew orchestrator (for merging/validation). | Realised by a central `CrewMemory` store (often a simple dict or a vector‚Äëstore of embeddings). The orchestrator injects this store into each agent‚Äôs prompt template under a reserved placeholder (e.g., `{crew_memory}`). Agents can append new entries or retrieve existing ones using semantic search. |\n",
      "| **3. Persistent (long‚Äëterm) memory** | Knowledge that survives across runs ‚Äì indexed documents, embeddings of past queries, evaluation metrics, and learned ‚Äúbest‚Äëpractice‚Äù query‚Äëenhancement patterns. | The orchestrator and any agent that explicitly requests it (e.g., a ‚Äúresearch‚Äù agent). | Backed by an external vector database (FAISS, Chroma, Pinecone, etc.) and a metadata store. CrewAI provides a `MemoryManager` component that abstracts CRUD operations (`add`, `search`, `update`, `delete`) so agents can treat it like a regular LLM tool. The manager also logs every enhanced query and its retrieval outcome for later analysis. |\n",
      "\n",
      "### How the pieces fit together\n",
      "\n",
      "1. **Task kickoff** ‚Äì The orchestrator creates a fresh `CrewMemory` and passes it (along with any relevant persistent snippets) to every agent‚Äôs prompt.\n",
      "2. **Agent execution** ‚Äì  \n",
      "   * An agent first reads from its **agent‚Äëlevel memory** to recall its own recent reasoning.  \n",
      "   * It then queries **crew‚Äëlevel memory** to see what other agents have already contributed (e.g., ‚ÄúHas anyone already fetched documents about LangChain agents?‚Äù).  \n",
      "   * If it needs deeper background, it calls the **persistent memory** via the `MemoryManager` (e.g., ‚ÄúRetrieve the last 5 successful query‚Äëenhancement patterns for ‚Äòembeddings‚Äô‚Äù).  \n",
      "   * After completing its sub‚Äëtask, it writes any new findings back to both its own short‚Äëterm store and the shared crew memory.\n",
      "3. **Orchestration & validation** ‚Äì When the crew finishes, the orchestrator can persist selected entries from `CrewMemory` into the long‚Äëterm store, enabling future runs to start with richer context and allowing automated analysis of recall/precision improvements.\n",
      "\n",
      "### Benefits of this structure\n",
      "\n",
      "* **Isolation with collaboration** ‚Äì Agents keep their own reasoning private (preventing accidental context leakage) while still being able to contribute to a common pool of knowledge.\n",
      "* **Scalability** ‚Äì Short‚Äëterm memory stays lightweight, so adding more agents does not blow up the prompt size; only the shared memory is serialized into the prompt when needed, and the heavy‚Äëweight persistent store lives outside the LLM.\n",
      "* **Feedback loop for query enhancement** ‚Äì Because every enhanced query and its retrieval result is logged in the persistent memory, a later ‚Äúoptimization‚Äù agent can query that history, spot patterns, and suggest better rewrites for upcoming tasks.\n",
      "\n",
      "In short, CrewAI offers a three‚Äëtiered memory architecture‚Äîagent‚Äëlevel short‚Äëterm, crew‚Äëlevel shared, and persistent long‚Äëterm‚Äîcoordinated by a `MemoryManager` and the central orchestrator, allowing multi‚Äëagent crews to remember, share, and build upon knowledge across both a single interaction and the entire lifespan of the system.\n",
      "\n",
      "Q: **In what ways do CrewAI‚Äôs agents differ from LangChain‚Äôs agents in terms of design, tool integration, and overall workflow?** \n",
      " : **CrewAI‚ÄØagents vs. LangChain‚ÄØagents**\n",
      "\n",
      "| Aspect | CrewAI agents | LangChain agents |\n",
      "|--------|---------------|-----------------|\n",
      "| **Design philosophy** | ‚Ä¢ **Role‚Äëcentric** ‚Äì each agent is given a specific job (e.g., ‚Äúquery‚Äëenhancer‚Äù, ‚Äúevaluator‚Äù, ‚Äúaggregator‚Äù). <br>‚Ä¢ Built for **task decomposition**: a higher‚Äëlevel ‚Äúcrew‚Äù orchestrates a sequence of specialized agents that work together on a single user request. <br>‚Ä¢ Emphasises **collaboration** and feedback loops between agents. | ‚Ä¢ **Component‚Äëcentric** ‚Äì an agent is a reusable LLM‚Äëdriven ‚Äútool‚Äù that can be plugged into any chain. <br>‚Ä¢ Designed as a **single‚Äëpurpose** unit (e.g., a web‚Äësearch agent, a calculator agent) that can be invoked wherever needed. <br>‚Ä¢ Focuses on providing the **building blocks** for pipelines rather than on explicit multi‚Äëagent orchestration. |\n",
      "| **Tool integration** | ‚Ä¢ Tools are **attached to the role** of an agent. A ‚Äúquery‚Äëenhancer‚Äù may have access to a retriever, a vector store, and a feedback logger, while a ‚Äúvalidator‚Äù may have a different set of tools. <br>‚Ä¢ CrewAI‚Äôs orchestration layer can **share tool instances** across agents, pass intermediate results, and dynamically switch tools based on evaluation metrics. <br>‚Ä¢ Supports **continuous refinement** ‚Äì an agent can re‚Äërun a tool (e.g., re‚Äëquery a vector DB) after receiving feedback from another agent. | ‚Ä¢ Tools are wrapped as **Tool objects** (retriever, web‚Äësearch, calculator, custom API) and passed to the agent at construction time. <br>‚Ä¢ The agent decides which tool to call based on the LLM‚Äôs output; there is no built‚Äëin mechanism for multiple agents to coordinate tool usage. <br>‚Ä¢ Tool usage is typically **static** for the life of the agent unless the developer rewrites the chain. |\n",
      "| **Overall workflow** | 1. **User input** ‚Üí ‚Äúdecomposer‚Äù agent breaks the request into sub‚Äëquestions. <br>2. Each sub‚Äëquestion is handed to a **specialized enhancer** agent that rewrites it (adds missing context, clarifies intent). <br>3. Enhanced queries are sent to retrieval/processing agents. <br>4. An **evaluator** agent ranks the results, possibly sending a query back for further refinement. <br>5. A **aggregator** agent synthesizes the final answer. <br>‚Ä¢ The crew can loop automatically (feedback ‚Üí re‚Äëenhance ‚Üí re‚Äëretrieve) until a quality threshold is met. | 1. **User input** ‚Üí a single LangChain agent receives the prompt. <br>2. The LLM decides (via a prompt template) whether to call a tool; if so, the tool is invoked. <br>3. The result is fed back into the same agent‚Äôs prompt and the cycle repeats until the agent decides it has an answer. <br>4. The output is returned directly to the user. <br>‚Ä¢ Any ‚Äúmulti‚Äëstep‚Äù behavior must be explicitly coded by the developer (e.g., by chaining several agents together). |\n",
      "| **Typical use‚Äëcase emphasis** | ‚Ä¢ Complex, ambiguous queries that benefit from **iterative clarification** and **multiple perspectives** (e.g., research assistants, multi‚Äëdomain QA). <br>‚Ä¢ Scenarios where you want the system to **learn from past runs** (logging enhanced queries, performance metrics). | ‚Ä¢ Straight‚Äëforward pipelines where a single LLM‚Äëdriven agent can **orchestrate a fixed set of tools** (e.g., ‚Äúsearch‚Äëthen‚Äësummarize‚Äù, ‚Äúcalculator‚Äëaugmented reasoning‚Äù). <br>‚Ä¢ Rapid prototyping of tool‚Äëaugmented agents without needing a full crew infrastructure. |\n",
      "| **Orchestration layer** | Provided by CrewAI: a **crew manager** that schedules agents, passes messages, and monitors success criteria. | Implicit in LangChain‚Äôs `Chain`/`SequentialChain`/`AgentExecutor`; orchestration is **manual** (you stitch together components yourself). |\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "- **Design:** CrewAI builds a *team* of role‚Äëspecific agents that decompose, enhance, evaluate, and aggregate; LangChain gives you *stand‚Äëalone* agents that you can chain yourself.  \n",
      "- **Tool integration:** CrewAI ties tools to agent roles and lets agents share and re‚Äëinvoke tools based on feedback; LangChain attaches a static set of tools to each agent and leaves coordination to the developer.  \n",
      "- **Workflow:** CrewAI runs a **multi‚Äëagent loop** (decompose ‚Üí enhance ‚Üí retrieve ‚Üí evaluate ‚Üí aggregate) with automatic refinement; LangChain runs a **single‚Äëagent loop** that calls tools as directed by the LLM, requiring explicit chaining for any extra steps.  \n",
      "\n",
      "These differences make CrewAI better suited for large‚Äëscale, iterative, multi‚Äëstep retrieval‚Äëor‚Äëreasoning tasks, while LangChain excels at rapid construction of modular, tool‚Äëaugmented agents.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does Langchain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"‚úÖ Final Answer: \\n\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_UDEMY (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
