{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06178248",
   "metadata": {},
   "source": [
    "## Query Enhancement - Query Expansion Techniques\n",
    "In a RAG pipeline, the quality of the query sent to the retriever determines how good the retrieved context is - and therefore, how accurate the LLM's final answer will be.\n",
    "That's where Query Expansion/ Enhancement comes in.\n",
    "\n",
    "### üéØ What is Query Enhancement? \n",
    "Query enhancement refers to techniques used to improve or reformulate the user query to retrieve better, more relevant documents from the knowledge base. It is especially useful when:\n",
    "- The original query is short, ambiguous , or under-specified\n",
    "- You want to broaden the scope to catch synonyms, related phrases, or spelling variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35c9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.document_loaders import TextLoader\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: TextLoader\n",
    "loader = TextLoader(\"langchain.txt\", encoding=\"utf-8\", autodetect_encoding=True)\n",
    "raw_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c7a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split the documents\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "chunks = splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d3718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Embedding model and vectore store\n",
    "embedding_model = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# make MMR retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type = \"mmr\",\n",
    "    search_kwargs = {\"k\":5}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929bd1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001F1608337A0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F160833CB0>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: LLM and Prompt\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = init_chat_model(\"groq:openai/gpt-oss-120b\")\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcb1e94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='You are a helpful assitant. Expand the following query to improve document retrieval by adding relevent synonyms, technical terms, and useful context.\\nOriginal query: \"{query}\" \\nExpanded query:                                                                                                                                                                    \\n')\n",
       "| ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001F1608337A0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F160833CB0>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query expansion\n",
    "query_expansion_prompt = PromptTemplate.from_template(\"\"\"You are a helpful assitant. Expand the following query to improve document retrieval by adding relevent synonyms, technical terms, and useful context.\n",
    "Original query: \"{query}\" \n",
    "Expanded query:                                                                                                                                                                    \n",
    "\"\"\")\n",
    "\n",
    "query_expansion_chain = query_expansion_prompt | llm | StrOutputParser()\n",
    "query_expansion_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b43f9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Expanded Search Query**\\n\\n```\\n(LangChain OR \"Langchain\") AND (memory OR \"context management\" OR \"conversation memory\" OR \"state persistence\" OR \"session memory\" OR \"chatbot memory\" OR \"memory buffer\" OR \"memory store\" OR \"memory module\") \\nAND ( \"ConversationBufferMemory\" OR \"ConversationSummaryMemory\" OR \"ConversationEntityMemory\" OR \"ConversationKGMemory\" OR \"ConversationReplayMemory\" OR \"ConversationChain\" OR \"LLMChain\" OR \"ChatPromptTemplate\" OR \"PromptTemplate\" ) \\nAND ( \"vector store\" OR \"vector database\" OR FAISS OR Chroma OR Pinecone OR Weaviate OR Milvus OR Qdrant OR \"embedding store\" ) \\nAND ( \"retrieval‚Äëaugmented generation\" OR RAG OR \"retrieval‚Äëaugmented\" OR \"knowledge retrieval\" ) \\nAND ( \"state management\" OR \"session state\" OR \"persistent storage\" OR Redis OR \"SQL database\" OR \"PostgreSQL\" OR \"MongoDB\" OR \"DynamoDB\" OR \"Azure Cosmos DB\" ) \\nAND ( \"LLM\" OR \"large language model\" OR \"OpenAI\" OR \"ChatGPT\" OR \"Claude\" OR \"Gemini\" OR \"Llama\" ) \\nAND ( \"prompt engineering\" OR \"prompt design\" OR \"chain of thought\" OR \"chain management\" )\\n```\\n\\n### How the Expansion Helps\\n| Original Term | Added Synonyms / Related Concepts | Why It‚Äôs Useful |\\n|---------------|----------------------------------|-----------------|\\n| **Langchain** | LangChain, Langchain, ‚ÄúLangChain framework‚Äù | Captures variations in spelling/capitalization and broader framework references. |\\n| **Memory** | context management, conversation memory, state persistence, session memory, memory buffer, memory store, memory module | Covers all ways the concept of ‚Äúmemory‚Äù is described in documentation and tutorials. |\\n| **Specific Memory Classes** | ConversationBufferMemory, ConversationSummaryMemory, ConversationEntityMemory, ConversationKGMemory, ConversationReplayMemory, ConversationChain, LLMChain | Directly targets the built‚Äëin memory implementations and related chain objects that users often search for. |\\n| **Vector Stores / Embedding Stores** | vector store, vector database, FAISS, Chroma, Pinecone, Weaviate, Milvus, Qdrant, embedding store | LangChain memory is frequently tied to vector‚Äëstore back‚Äëends for storing embeddings and retrieval. |\\n| **Retrieval‚ÄëAugmented Generation** | RAG, retrieval‚Äëaugmented, knowledge retrieval | Many memory use‚Äëcases involve RAG pipelines; adding these terms surfaces relevant architecture discussions. |\\n| **Persistence Layers** | state management, session state, persistent storage, Redis, SQL database, PostgreSQL, MongoDB, DynamoDB, Azure Cosmos DB | Memory can be backed by various storage solutions; including them widens the search to implementation guides. |\\n| **LLM Context** | LLM, large language model, OpenAI, ChatGPT, Claude, Gemini, Llama | Memory is used to feed context to LLMs, so linking the two improves relevance. |\\n| **Prompt‚ÄëRelated** | prompt engineering, prompt design, chain of thought, chain management | Memory often interacts with prompt templates; these terms capture that intersection. |\\n\\n### Tips for Using the Expanded Query\\n- **Boolean Logic:** Most search engines understand `AND`, `OR`, and parentheses. Adjust the syntax if you‚Äôre using a specific engine (e.g., Google, Elasticsearch, Pinecone console).  \\n- **Focus/Filter:** If you get too many results, start by narrowing the query‚Äîe.g., keep only the first three groups to focus on ‚ÄúLangChain memory implementations‚Äù.  \\n- **Quote Exact Phrases:** Keep the quoted terms (e.g., `\"ConversationBufferMemory\"`) to prioritize official class names.  \\n- **Add Site‚ÄëSpecific Filters:** For official docs, append `site:langchain.com` or `site:github.com/langchain-ai`.  \\n\\nFeel free to mix‚Äëand‚Äëmatch the clauses above to suit the depth of information you need!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {\"query\": \"Langchain memory\"}\n",
    "query_expansion_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e54d669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG answering prompt\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the question based on the context below.\n",
    "Context : {context}\n",
    "Qusetion: {input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, answer_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89d2041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Rag pipeline with query expansion\n",
    "rag_pipeline = (\n",
    "    RunnableMap({\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"context\": lambda x : retriever.invoke(query_expansion_chain.invoke({\"query\": x[\"input\"]}))\n",
    "    })\n",
    "    | document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b8be7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Expanded query**\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"input\": \"What kinds of memory or data storage does the CrewAI platform support? Specifically, which types of short‚Äëterm (in‚Äëmemory, cache, session) and long‚Äëterm (persistent, vector store, knowledge‚Äëbase, database) memory are available? Does CrewAI provide support for semantic memory (embeddings), episodic memory (conversation history), persistent vector databases (e.g., Pinecone, Weaviate, Milvus), relational storage (PostgreSQL, MySQL), NoSQL stores (MongoDB, Redis), file‚Äëbased storage, or any custom memory back‚Äëends? Please include any technical terms such as ‚Äòvector store‚Äô, ‚Äòembedding cache‚Äô, ‚Äòknowledge graph‚Äô, ‚Äòstate management‚Äô, ‚Äòpersistent store‚Äô, ‚Äòephemeral cache‚Äô, and relevant APIs or configuration options for selecting or extending memory types in CrewAI.\"\n",
      "}\n",
      "```\n",
      "Ansewer:\n",
      " CrewAI inherits the same flexible‚ÄØ*memory*‚ÄØabstractions that LangChain provides, so you can plug in any of the standard memory types that LangChain supports. In practice this means that a CrewAI‚Äëbased multi‚Äëagent pipeline can use:\n",
      "\n",
      "| Memory type | What it does | Typical use‚Äëcase in CrewAI |\n",
      "|-------------|--------------|---------------------------|\n",
      "| **ConversationBufferMemory** (in‚Äëmemory buffer) | Stores the raw sequence of user‚Äëassistant turns. | Simple chat‚Äëstyle agents that need to keep the recent dialogue context. |\n",
      "| **ConversationSummaryMemory** | Summarises the conversation as it grows, keeping a compact representation. | Long‚Äërunning agents where the full transcript would become too large. |\n",
      "| **VectorStoreRetrieverMemory** (vector‚Äëstore‚Äëbacked) | Persists embeddings of past interactions in a vector DB (FAISS, Pinecone, Chroma, etc.) and retrieves relevant chunks later. | Agents that need semantic recall of earlier facts or documents. |\n",
      "| **CombinedMemory** (hybrid) | Chains together two or more memory modules (e.g., buffer‚ÄØ+‚ÄØvector store) so the agent can fall back to the most appropriate source. | Complex pipelines where both recent turn‚Äëby‚Äëturn context and long‚Äëterm semantic recall are required. |\n",
      "| **Redis/SQL‚Äëbacked Memory** | Persists memory in an external key‚Äëvalue store or relational DB. | Production‚Äëgrade deployments where durability across restarts is needed. |\n",
      "\n",
      "Because CrewAI‚Äôs architecture treats memory as a pluggable component, you can also bring your own custom memory class (e.g., a time‚Äëweighted decay memory, a knowledge‚Äëgraph memory, etc.) as long as it follows the same interface that LangChain expects. In short, CrewAI supports **any LangChain‚Äëcompatible memory**, with the most common out‚Äëof‚Äëthe‚Äëbox options being the buffer, summary, vector‚Äëstore, and hybrid memories listed above.\n"
     ]
    }
   ],
   "source": [
    "# Run Query\n",
    "# query = {\"input\": \"What types of memeory does Langchain support?\"}\n",
    "query = {\"input\": \"What types of memeory does CrewAI support?\"}\n",
    "print(query_expansion_chain.invoke({\"query\": query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"Ansewer:\\n\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_UDEMY (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
